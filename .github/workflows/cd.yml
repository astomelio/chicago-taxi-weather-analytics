name: CD Pipeline - Deploy Infrastructure

on:
  push:
    branches: [ main ]
    paths:
      - 'terraform/**'
      - 'functions/**'
      - '.github/workflows/cd.yml'
  workflow_dispatch:  # Permite ejecuci√≥n manual

env:
  TF_VERSION: 1.5.0
  PYTHON_VERSION: '3.9'

jobs:
  prepare-function-zip:
    name: Prepare Cloud Function ZIP
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Create function ZIP
        run: |
          cd functions/weather_ingestion
          zip -r ../../terraform/weather-ingestion-source.zip . \
            -x "*.pyc" "__pycache__/*" "*.git*" "*.zip"
      
      - name: Upload ZIP artifact
        uses: actions/upload-artifact@v4
        with:
          name: function-zip
          path: terraform/weather-ingestion-source.zip
          retention-days: 1

  deploy-terraform:
    name: Deploy Infrastructure with Terraform
    runs-on: ubuntu-latest
    needs: prepare-function-zip
    defaults:
      run:
        working-directory: ./terraform
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Download function ZIP
        uses: actions/download-artifact@v4
        with:
          name: function-zip
          path: terraform/
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false
      
      - name: Setup GCP Auth
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Setup GCP Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
      
      - name: Enable Required APIs
        run: |
          gcloud services enable \
            bigquery.googleapis.com \
            cloudfunctions.googleapis.com \
            cloudscheduler.googleapis.com \
            storage.googleapis.com \
            datacatalog.googleapis.com \
            cloudbuild.googleapis.com \
            run.googleapis.com \
            --project=${{ secrets.GCP_PROJECT_ID }}
      
      - name: Create Terraform State Bucket
        run: |
          STATE_BUCKET="${{ secrets.GCP_PROJECT_ID }}-terraform-state"
          if ! gsutil ls -b gs://$STATE_BUCKET 2>/dev/null; then
            echo "üì¶ Creando bucket para estado de Terraform..."
            gsutil mb -p ${{ secrets.GCP_PROJECT_ID }} -l ${{ secrets.GCP_REGION || 'us-central1' }} gs://$STATE_BUCKET
            gsutil versioning set on gs://$STATE_BUCKET
            echo "‚úÖ Bucket creado: $STATE_BUCKET"
          else
            echo "‚úÖ Bucket ya existe: $STATE_BUCKET"
          fi
      
      - name: Terraform Init
        run: |
          STATE_BUCKET="${{ secrets.GCP_PROJECT_ID }}-terraform-state"
          terraform init \
            -backend-config="bucket=$STATE_BUCKET" \
            -backend-config="prefix=terraform/state" \
            -reconfigure
      
      - name: Force Unlock Terraform State (if locked)
        run: |
          echo "üîì Verificando si hay locks en el estado de Terraform..."
          STATE_BUCKET="${{ secrets.GCP_PROJECT_ID }}-terraform-state"
          LOCK_FILE="gs://${STATE_BUCKET}/terraform/state/default.tflock"
          
          # Verificar si el archivo de lock existe
          if gsutil stat "$LOCK_FILE" &>/dev/null; then
            echo "‚ö†Ô∏è  Lock detectado. Intentando leer informaci√≥n del lock..."
            LOCK_CONTENT=$(gsutil cat "$LOCK_FILE" 2>/dev/null || echo "")
            
            # Extraer el Lock ID del contenido
            LOCK_ID=$(echo "$LOCK_CONTENT" | grep -E "^\s*ID:\s*[0-9]+" | sed -E 's/^\s*ID:\s*([0-9]+).*/\1/' | head -1 || echo "")
            
            if [ -n "$LOCK_ID" ]; then
              echo "üîì Lock ID encontrado: $LOCK_ID. Forzando unlock..."
              terraform force-unlock -force "$LOCK_ID" || {
                echo "‚ö†Ô∏è  No se pudo forzar unlock con terraform. Intentando eliminar el lock directamente..."
                # Como √∫ltimo recurso, eliminar el archivo de lock directamente
                gsutil rm "$LOCK_FILE" 2>/dev/null && echo "‚úÖ Lock eliminado directamente" || echo "‚ö†Ô∏è  No se pudo eliminar el lock"
              }
            else
              echo "‚ö†Ô∏è  Lock detectado pero no se pudo extraer el ID. Eliminando lock directamente..."
              gsutil rm "$LOCK_FILE" 2>/dev/null && echo "‚úÖ Lock eliminado" || echo "‚ö†Ô∏è  No se pudo eliminar el lock"
            fi
          else
            echo "‚úÖ No hay locks activos"
          fi
        continue-on-error: true
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
      
      - name: Import existing resources
        run: |
          echo "üîÑ Verificando y importando recursos existentes..."
          
          # Funci√≥n para importar si el recurso existe
          import_if_exists() {
            local resource=$1
            local import_id=$2
            local resource_name=$3
            
            # Verificar si el recurso ya est√° en el estado
            if terraform state show "$resource" &>/dev/null; then
              echo "‚úì $resource_name ya est√° en el estado"
              return 0
            fi
            
            # Intentar importar
            echo "üîÑ Intentando importar $resource_name..."
            IMPORT_OUTPUT=$(terraform import "$resource" "$import_id" 2>&1)
            IMPORT_EXIT=$?
            if [ $IMPORT_EXIT -eq 0 ]; then
              echo "‚úÖ Importado: $resource_name"
              return 0
            else
              # Si el error es que ya existe, intentar forzar el import
              if echo "$IMPORT_OUTPUT" | grep -q "already exists\|Already Exists\|Resource already managed"; then
                echo "‚ö†Ô∏è  $resource_name ya existe. Intentando forzar import..."
                # El recurso existe pero no est√° en el estado, intentar importar de nuevo con -lock=false
                terraform import -lock=false "$resource" "$import_id" 2>&1 || echo "   No se pudo forzar import"
              else
                echo "‚ö†Ô∏è  No se pudo importar $resource_name: $IMPORT_OUTPUT"
              fi
              return 0  # No fallar
            fi
          }
          
          # Importar datasets de BigQuery
          import_if_exists "google_bigquery_dataset.raw_dataset" "${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_raw" "Dataset raw"
          import_if_exists "google_bigquery_dataset.silver_dataset" "${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_silver" "Dataset silver"
          import_if_exists "google_bigquery_dataset.gold_dataset" "${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_gold" "Dataset gold"
          
          # Importar service account
          import_if_exists "google_service_account.weather_ingestion_sa" "projects/${{ secrets.GCP_PROJECT_ID }}/serviceAccounts/weather-ingestion-sa@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com" "Service account"
          
          # Importar storage bucket
          import_if_exists "google_storage_bucket.function_source" "${{ secrets.GCP_PROJECT_ID }}-function-source" "Storage bucket"
          
          # Intentar importar taxonomy (m√°s complejo)
          TAXONOMY_NAME=$(gcloud data-catalog taxonomies list --location=${{ secrets.GCP_REGION || 'us-central1' }} --format='value(name)' --project=${{ secrets.GCP_PROJECT_ID }} 2>/dev/null | head -1 | xargs basename 2>/dev/null || echo "")
          if [ -n "$TAXONOMY_NAME" ]; then
            import_if_exists "google_data_catalog_taxonomy.sensitive_data" "projects/${{ secrets.GCP_PROJECT_ID }}/locations/${{ secrets.GCP_REGION || 'us-central1' }}/taxonomies/$TAXONOMY_NAME" "Data Catalog Taxonomy"
          else
            echo "‚ö†Ô∏è  Taxonomy no encontrada (se crear√° si es necesario)"
          fi
          
          echo "‚úÖ Proceso de import completado"
          echo "üìä Estado actual:"
          terraform state list || echo "Estado vac√≠o o no disponible"
          echo "üíæ Sincronizando estado con backend remoto..."
          terraform force-unlock -force $(terraform state list | wc -l) 2>/dev/null || true
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_developer_email: ${{ secrets.DEVELOPER_EMAIL }}
          TF_VAR_openweather_api_key: ${{ secrets.OPENWEATHER_API_KEY || '' }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
          TF_VAR_chicago_latitude: 41.8781
          TF_VAR_chicago_longitude: -87.6298
        continue-on-error: true
      
      - name: Terraform Plan
        run: terraform plan -out=tfplan -lock-timeout=5m
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_developer_email: ${{ secrets.DEVELOPER_EMAIL }}
          TF_VAR_openweather_api_key: ${{ secrets.OPENWEATHER_API_KEY || '' }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
          TF_VAR_chicago_latitude: 41.8781
          TF_VAR_chicago_longitude: -87.6298
        continue-on-error: false
      
      - name: Terraform Apply
        run: |
          echo "üîÑ Aplicando cambios de Terraform..."
          # Intentar aplicar el plan con timeout para el lock
          if terraform apply -auto-approve -lock-timeout=5m tfplan 2>&1 | tee /tmp/apply_output.log; then
            echo "‚úÖ Terraform apply exitoso"
            exit 0
          else
            APPLY_EXIT=$?
            APPLY_OUTPUT=$(cat /tmp/apply_output.log)
            
            # Si el error es por recursos existentes, intentar importarlos y re-aplicar
            if echo "$APPLY_OUTPUT" | grep -q "Already Exists\|already exists\|Error 409"; then
              echo "‚ö†Ô∏è  Detectados recursos existentes. Intentando importar y re-aplicar..."
              
              # Re-importar recursos que fallaron
              terraform import google_bigquery_dataset.raw_dataset ${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_raw 2>/dev/null || true
              terraform import google_bigquery_dataset.silver_dataset ${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_silver 2>/dev/null || true
              terraform import google_bigquery_dataset.gold_dataset ${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_gold 2>/dev/null || true
              terraform import google_service_account.weather_ingestion_sa projects/${{ secrets.GCP_PROJECT_ID }}/serviceAccounts/weather-ingestion-sa@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com 2>/dev/null || true
              terraform import google_storage_bucket.function_source ${{ secrets.GCP_PROJECT_ID }}-function-source 2>/dev/null || true
              
              # Re-aplicar sin el plan (para que use el estado actualizado)
              echo "üîÑ Re-aplicando con estado actualizado..."
              terraform apply -auto-approve -lock-timeout=5m || {
                echo "‚ö†Ô∏è  Re-apply tambi√©n fall√≥, pero algunos recursos pueden haberse creado"
                exit 0  # No fallar completamente
              }
            else
              echo "‚ùå Terraform apply fall√≥ con error diferente:"
              echo "$APPLY_OUTPUT"
              exit $APPLY_EXIT
            fi
          fi
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_developer_email: ${{ secrets.DEVELOPER_EMAIL }}
          TF_VAR_openweather_api_key: ${{ secrets.OPENWEATHER_API_KEY || '' }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
          TF_VAR_chicago_latitude: 41.8781
          TF_VAR_chicago_longitude: -87.6298
        continue-on-error: false
      
      - name: Terraform Output
        run: terraform output
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_developer_email: ${{ secrets.DEVELOPER_EMAIL }}
          TF_VAR_openweather_api_key: ${{ secrets.OPENWEATHER_API_KEY || '' }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
          TF_VAR_chicago_latitude: 41.8781
          TF_VAR_chicago_longitude: -87.6298

  ingest-historical-weather:
    name: Ingest Historical Weather Data
    runs-on: ubuntu-latest
    needs: deploy-terraform
    defaults:
      run:
        working-directory: ./
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install google-cloud-bigquery requests
      
      - name: Setup GCP Auth
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Setup GCP Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
      
      - name: Get Cloud Function URL
        id: get-function-url
        run: |
          FUNCTION_URL=$(gcloud functions describe weather-ingestion \
            --region=${{ secrets.GCP_REGION || 'us-central1' }} \
            --gen2 \
            --format="value(serviceConfig.uri)" \
            --project=${{ secrets.GCP_PROJECT_ID }})
          echo "url=$FUNCTION_URL" >> $GITHUB_OUTPUT
        continue-on-error: false
      
      - name: Check if historical data exists
        id: check-historical
        run: |
          python3 << 'PYEOF'
          from google.cloud import bigquery
          import os
          
          project_id = os.environ['GCP_PROJECT_ID']
          client = bigquery.Client(project=project_id)
          
          # Verificar si ya existen datos hist√≥ricos (junio-diciembre 2023)
          query = f"""
          SELECT COUNT(*) as count 
          FROM `{project_id}.chicago_taxi_raw.weather_data`
          WHERE date >= '2023-06-01' AND date <= '2023-12-31'
          """
          
          try:
              result = client.query(query).result()
              count = next(result).count
              
              print(f"üìä Datos hist√≥ricos encontrados: {count} d√≠as")
              
              # Si hay al menos 180 d√≠as (6 meses), considerar que los datos hist√≥ricos ya existen
              if count >= 180:
                  print("‚úÖ Datos hist√≥ricos ya existen. Saltando ingesta hist√≥rica.")
                  with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                      f.write("needs_historical=false\n")
              else:
                  print(f"‚ö†Ô∏è  Solo hay {count} d√≠as. Necesitamos ingesta hist√≥rica.")
                  with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                      f.write("needs_historical=true\n")
          except Exception as e:
              print(f"‚ö†Ô∏è  Error verificando datos: {e}. Asumiendo que necesitamos ingesta hist√≥rica.")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write("needs_historical=true\n")
          PYEOF
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GITHUB_OUTPUT: ${{ github.output }}
      
      - name: Trigger Historical Ingestion
        if: steps.check-historical.outputs.needs_historical == 'true'
        run: |
          FUNCTION_URL="${{ steps.get-function-url.outputs.url }}"
          echo "üîÑ Triggering historical ingestion at: $FUNCTION_URL"
          
          TOKEN=$(gcloud auth print-identity-token)
          
          curl -X POST "$FUNCTION_URL" \
            -H "Authorization: Bearer $TOKEN" \
            -H "Content-Type: application/json" \
            -d '{"historical": true}' \
            --max-time 900 \
            --fail-with-body \
            --show-error
          
          echo "‚úÖ Historical ingestion completed"
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
      
      - name: Skip Historical Ingestion
        if: steps.check-historical.outputs.needs_historical == 'false'
        run: |
          echo "‚è≠Ô∏è  Datos hist√≥ricos ya existen. Saltando ingesta hist√≥rica."

  deploy-dbt:
    name: Deploy dbt Models
    runs-on: ubuntu-latest
    needs: [deploy-terraform, ingest-historical-weather]
    defaults:
      run:
        working-directory: ./dbt
    
    # Esperar un poco para que los datos de clima se propaguen
    timeout-minutes: 30
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dbt and dependencies
        run: |
          pip install dbt-bigquery
          pip install google-cloud-bigquery
      
      - name: Setup GCP Auth
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Setup dbt profiles directory
        run: |
          mkdir -p ~/.dbt
          if [ -f dbt/profiles.yml ]; then
            cp dbt/profiles.yml ~/.dbt/profiles.yml
          else
            echo "‚ö†Ô∏è  dbt/profiles.yml no encontrado, creando uno b√°sico..."
            cat > ~/.dbt/profiles.yml << 'PROFEOF'
          chicago_taxi_analysis:
            target: dev
            outputs:
              dev:
                type: bigquery
                method: oauth
                project: "{{ env_var('GCP_PROJECT_ID') }}"
                dataset: "{{ env_var('DBT_DATASET', 'chicago_taxi_silver') }}"
                location: us-central1
                threads: 4
                timeout_seconds: 300
                priority: interactive
                maximum_bytes_billed: 1000000000
          PROFEOF
          fi
          echo "‚úÖ dbt profiles configurado en ~/.dbt/profiles.yml"
      
      - name: Wait for weather data
        run: |
          echo "‚è≥ Esperando a que los datos de clima est√©n disponibles..."
          python3 << 'PYEOF'
          from google.cloud import bigquery
          import time
          import os
          
          project_id = os.environ['GCP_PROJECT_ID']
          client = bigquery.Client(project=project_id)
          
          max_wait = 300  # 5 minutos
          wait_time = 0
          check_interval = 10
          
          while wait_time < max_wait:
              try:
                  query = f"SELECT COUNT(*) as count FROM `{project_id}.chicago_taxi_raw.weather_data`"
                  result = client.query(query).result()
                  count = next(result).count
                  if count > 0:
                      print(f"‚úÖ Datos de clima disponibles: {count} registros")
                      exit(0)
              except Exception as e:
                  pass
              time.sleep(check_interval)
              wait_time += check_interval
              print(f"   Esperando... ({wait_time}s)")
          
          print("‚ö†Ô∏è  Timeout esperando datos de clima")
          exit(1)
          PYEOF
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
      
      - name: Run dbt models
        run: |
          dbt deps --profiles-dir ~/.dbt
          echo "üîÑ Ejecutando modelos silver..."
          dbt run --models silver --profiles-dir ~/.dbt
          echo "üîÑ Ejecutando modelos gold..."
          dbt run --models gold --profiles-dir ~/.dbt
          dbt test --profiles-dir ~/.dbt
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          DBT_DATASET: chicago_taxi_silver
      
      - name: Verify Data
        run: |
          echo "Verifying data in BigQuery..."
          python3 << 'PYEOF'
          from google.cloud import bigquery
          import os
          
          client = bigquery.Client(project=os.environ['GCP_PROJECT_ID'])
          
          # Verificar datos en silver
          query_silver = """
          SELECT COUNT(*) as count 
          FROM `{}.chicago_taxi_silver.taxi_trips_silver`
          """.format(os.environ['GCP_PROJECT_ID'])
          
          result_silver = client.query(query_silver).result()
          count_silver = next(result_silver).count
          print(f"‚úÖ Silver layer: {count_silver:,} taxi trips")
          
          # Verificar datos en gold
          query_gold = """
          SELECT COUNT(*) as count 
          FROM `{}.chicago_taxi_gold.daily_summary`
          """.format(os.environ['GCP_PROJECT_ID'])
          
          result_gold = client.query(query_gold).result()
          count_gold = next(result_gold).count
          print(f"‚úÖ Gold layer: {count_gold:,} daily summaries")
          
          # Verificar datos de clima
          query_weather = """
          SELECT COUNT(*) as count 
          FROM `{}.chicago_taxi_raw.weather_data`
          """.format(os.environ['GCP_PROJECT_ID'])
          
          result_weather = client.query(query_weather).result()
          count_weather = next(result_weather).count
          print(f"‚úÖ Weather data: {count_weather} days")
          
          if count_silver > 0 and count_gold > 0 and count_weather > 0:
            print("\n‚úÖ All data layers populated successfully!")
            exit(0)
          else:
            print("\n‚ö†Ô∏è  Some data layers are empty")
            exit(1)
          PYEOF
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
      
      - name: Prepare Looker Studio Dashboard
        run: |
          echo "üé® Preparing Looker Studio dashboard template..."
          python3 << 'PYEOF'
          import json
          import os
          
          project_id = os.environ['GCP_PROJECT_ID']
          
          # Generar template del dashboard
          template = {
              "version": "1.0",
              "projectId": project_id,
              "dataSource": {
                  "type": "BIGQUERY",
                  "projectId": project_id,
                  "datasetId": "chicago_taxi_gold",
                  "tableId": "daily_summary"
              },
              "instructions": {
                  "step1": f"Go to https://lookerstudio.google.com/",
                  "step2": "Click 'Create' > 'Report'",
                  "step3": f"Add data source: BigQuery > {project_id} > chicago_taxi_gold > daily_summary",
                  "step4": "Follow CREAR_DASHBOARD.md for detailed visualizations"
              }
          }
          
          with open('looker_dashboard_template.json', 'w') as f:
              json.dump(template, f, indent=2)
          
          print("‚úÖ Dashboard template created: looker_dashboard_template.json")
          print(f"üìä Connect to: {project_id}.chicago_taxi_gold.daily_summary")
          PYEOF
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
