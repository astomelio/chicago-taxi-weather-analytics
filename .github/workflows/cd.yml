name: CD Pipeline - Deploy Infrastructure

on:
  push:
    branches: [ main ]
    paths:
      - 'terraform/**'
      - 'functions/**'
      - 'airflow/**'
      - 'dbt/**'
      - '.github/workflows/cd.yml'
  workflow_dispatch:  # Permite ejecuci√≥n manual

env:
  TF_VERSION: 1.5.0
  PYTHON_VERSION: '3.9'

jobs:
  prepare-function-zip:
    name: Prepare Cloud Function ZIP
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Create function ZIP
        run: |
          cd functions/weather_ingestion
          zip -r ../../terraform/weather-ingestion-source.zip . \
            -x "*.pyc" "__pycache__/*" "*.git*" "*.zip"
      
      - name: Upload ZIP artifact
        uses: actions/upload-artifact@v4
        with:
          name: function-zip
          path: terraform/weather-ingestion-source.zip
          retention-days: 1

  deploy-terraform:
    name: Deploy Infrastructure with Terraform
    runs-on: ubuntu-latest
    needs: prepare-function-zip
    defaults:
      run:
        working-directory: ./terraform
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Download function ZIP
        uses: actions/download-artifact@v4
        with:
          name: function-zip
          path: terraform/
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false
      
      - name: Setup GCP Auth
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Setup GCP Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
      
      - name: Enable Required APIs
        run: |
          gcloud services enable \
            bigquery.googleapis.com \
            cloudfunctions.googleapis.com \
            cloudscheduler.googleapis.com \
            storage.googleapis.com \
            datacatalog.googleapis.com \
            cloudbuild.googleapis.com \
            run.googleapis.com \
            --project=${{ secrets.GCP_PROJECT_ID }}
      
      - name: Create Terraform State Bucket
        run: |
          STATE_BUCKET="${{ secrets.GCP_PROJECT_ID }}-terraform-state"
          if ! gsutil ls -b gs://$STATE_BUCKET 2>/dev/null; then
            echo "üì¶ Creando bucket para estado de Terraform..."
            gsutil mb -p ${{ secrets.GCP_PROJECT_ID }} -l ${{ secrets.GCP_REGION || 'us-central1' }} gs://$STATE_BUCKET
            gsutil versioning set on gs://$STATE_BUCKET
            echo "‚úÖ Bucket creado: $STATE_BUCKET"
          else
            echo "‚úÖ Bucket ya existe: $STATE_BUCKET"
          fi
      
      - name: Terraform Init
        run: |
          STATE_BUCKET="${{ secrets.GCP_PROJECT_ID }}-terraform-state"
          terraform init \
            -backend-config="bucket=$STATE_BUCKET" \
            -backend-config="prefix=terraform/state" \
            -reconfigure
      
      - name: Force Unlock Terraform State (if locked)
        run: |
          echo "üîì Verificando si hay locks en el estado de Terraform..."
          STATE_BUCKET="${{ secrets.GCP_PROJECT_ID }}-terraform-state"
          LOCK_FILE="gs://${STATE_BUCKET}/terraform/state/default.tflock"
          
          # Verificar si el archivo de lock existe
          if gsutil stat "$LOCK_FILE" &>/dev/null; then
            echo "‚ö†Ô∏è  Lock detectado. Intentando leer informaci√≥n del lock..."
            LOCK_CONTENT=$(gsutil cat "$LOCK_FILE" 2>/dev/null || echo "")
            
            # Extraer el Lock ID del contenido
            LOCK_ID=$(echo "$LOCK_CONTENT" | grep -E "^\s*ID:\s*[0-9]+" | sed -E 's/^\s*ID:\s*([0-9]+).*/\1/' | head -1 || echo "")
            
            if [ -n "$LOCK_ID" ]; then
              echo "üîì Lock ID encontrado: $LOCK_ID. Forzando unlock..."
              terraform force-unlock -force "$LOCK_ID" || {
                echo "‚ö†Ô∏è  No se pudo forzar unlock con terraform. Intentando eliminar el lock directamente..."
                # Como √∫ltimo recurso, eliminar el archivo de lock directamente
                gsutil rm "$LOCK_FILE" 2>/dev/null && echo "‚úÖ Lock eliminado directamente" || echo "‚ö†Ô∏è  No se pudo eliminar el lock"
              }
            else
              echo "‚ö†Ô∏è  Lock detectado pero no se pudo extraer el ID. Eliminando lock directamente..."
              gsutil rm "$LOCK_FILE" 2>/dev/null && echo "‚úÖ Lock eliminado" || echo "‚ö†Ô∏è  No se pudo eliminar el lock"
            fi
          else
            echo "‚úÖ No hay locks activos"
          fi
        continue-on-error: true
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
      
      - name: Import existing resources
        run: |
          set +e  # Desactivar modo de error estricto para este step
          echo "üîÑ Verificando y importando recursos existentes..."
          
          # Funci√≥n para importar si el recurso existe
          import_if_exists() {
            local resource=$1
            local import_id=$2
            local resource_name=$3
            
            # Verificar si el recurso ya est√° en el estado
            if terraform state show "$resource" &>/dev/null; then
              echo "‚úì $resource_name ya est√° en el estado"
              return 0
            fi
            
            # Intentar importar
            echo "üîÑ Intentando importar $resource_name..."
            IMPORT_OUTPUT=$(terraform import "$resource" "$import_id" 2>&1)
            IMPORT_EXIT=$?
            if [ $IMPORT_EXIT -eq 0 ]; then
              echo "‚úÖ Importado: $resource_name"
              return 0
            else
              # Si el error es que ya existe, intentar forzar el import
              if echo "$IMPORT_OUTPUT" | grep -q "already exists\|Already Exists\|Resource already managed"; then
                echo "‚ö†Ô∏è  $resource_name ya existe. Intentando forzar import..."
                # El recurso existe pero no est√° en el estado, intentar importar de nuevo con -lock=false
                terraform import -lock=false "$resource" "$import_id" 2>&1 || echo "   No se pudo forzar import"
              else
                echo "‚ö†Ô∏è  No se pudo importar $resource_name (esto es OK si el recurso no existe a√∫n):"
                echo "   $IMPORT_OUTPUT"
              fi
              return 0  # No fallar
            fi
          }
          
          # Importar datasets de BigQuery
          import_if_exists "google_bigquery_dataset.raw_dataset" "${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_raw" "Dataset raw"
          import_if_exists "google_bigquery_dataset.silver_dataset" "${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_silver" "Dataset silver"
          import_if_exists "google_bigquery_dataset.gold_dataset" "${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_gold" "Dataset gold"
          
          # Importar service account
          import_if_exists "google_service_account.weather_ingestion_sa" "projects/${{ secrets.GCP_PROJECT_ID }}/serviceAccounts/weather-ingestion-sa@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com" "Service account"
          
          # Importar storage bucket
          import_if_exists "google_storage_bucket.function_source" "${{ secrets.GCP_PROJECT_ID }}-function-source" "Storage bucket"
          
          # Intentar importar taxonomy (m√°s complejo)
          TAXONOMY_NAME=$(gcloud data-catalog taxonomies list --location=${{ secrets.GCP_REGION || 'us-central1' }} --format='value(name)' --project=${{ secrets.GCP_PROJECT_ID }} 2>/dev/null | head -1 | xargs basename 2>/dev/null || echo "")
          if [ -n "$TAXONOMY_NAME" ]; then
            import_if_exists "google_data_catalog_taxonomy.sensitive_data" "projects/${{ secrets.GCP_PROJECT_ID }}/locations/${{ secrets.GCP_REGION || 'us-central1' }}/taxonomies/$TAXONOMY_NAME" "Data Catalog Taxonomy"
          else
            echo "‚ö†Ô∏è  Taxonomy no encontrada (se crear√° si es necesario)"
          fi
          
          echo "‚úÖ Proceso de import completado"
          echo "üìä Estado actual:"
          terraform state list || echo "Estado vac√≠o o no disponible"
          echo "üíæ Sincronizando estado con backend remoto..."
          terraform force-unlock -force $(terraform state list | wc -l) 2>/dev/null || true
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_developer_email: ${{ secrets.DEVELOPER_EMAIL }}
          TF_VAR_openweather_api_key: ${{ secrets.OPENWEATHER_API_KEY || '' }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
          TF_VAR_chicago_latitude: 41.8781
          TF_VAR_chicago_longitude: -87.6298
        continue-on-error: true
      
      - name: Terraform Plan
        run: terraform plan -out=tfplan -lock-timeout=5m
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_developer_email: ${{ secrets.DEVELOPER_EMAIL }}
          TF_VAR_openweather_api_key: ${{ secrets.OPENWEATHER_API_KEY || '' }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
          TF_VAR_chicago_latitude: 41.8781
          TF_VAR_chicago_longitude: -87.6298
        continue-on-error: false
      
      - name: Terraform Apply
        run: |
          echo "üîÑ Aplicando cambios de Terraform..."
          # Intentar aplicar el plan con timeout para el lock
          if terraform apply -auto-approve -lock-timeout=5m tfplan 2>&1 | tee /tmp/apply_output.log; then
            echo "‚úÖ Terraform apply exitoso"
            exit 0
          else
            APPLY_EXIT=$?
            APPLY_OUTPUT=$(cat /tmp/apply_output.log)
            
            # Si el error es por recursos existentes, intentar importarlos y re-aplicar
            if echo "$APPLY_OUTPUT" | grep -q "Already Exists\|already exists\|Error 409"; then
              echo "‚ö†Ô∏è  Detectados recursos existentes. Intentando importar y re-aplicar..."
              
              # Re-importar recursos que fallaron
              terraform import google_bigquery_dataset.raw_dataset ${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_raw 2>/dev/null || true
              terraform import google_bigquery_dataset.silver_dataset ${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_silver 2>/dev/null || true
              terraform import google_bigquery_dataset.gold_dataset ${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_gold 2>/dev/null || true
              terraform import google_service_account.weather_ingestion_sa projects/${{ secrets.GCP_PROJECT_ID }}/serviceAccounts/weather-ingestion-sa@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com 2>/dev/null || true
              terraform import google_storage_bucket.function_source ${{ secrets.GCP_PROJECT_ID }}-function-source 2>/dev/null || true
              
              # Re-aplicar sin el plan (para que use el estado actualizado)
              echo "üîÑ Re-aplicando con estado actualizado..."
              terraform apply -auto-approve -lock-timeout=5m || {
                echo "‚ö†Ô∏è  Re-apply tambi√©n fall√≥, pero algunos recursos pueden haberse creado"
                exit 0  # No fallar completamente
              }
            else
              echo "‚ùå Terraform apply fall√≥ con error diferente:"
              echo "$APPLY_OUTPUT"
              exit $APPLY_EXIT
            fi
          fi
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_developer_email: ${{ secrets.DEVELOPER_EMAIL }}
          TF_VAR_openweather_api_key: ${{ secrets.OPENWEATHER_API_KEY || '' }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
          TF_VAR_chicago_latitude: 41.8781
          TF_VAR_chicago_longitude: -87.6298
        continue-on-error: false
      
      - name: Terraform Output
        run: terraform output
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_developer_email: ${{ secrets.DEVELOPER_EMAIL }}
          TF_VAR_openweather_api_key: ${{ secrets.OPENWEATHER_API_KEY || '' }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
          TF_VAR_chicago_latitude: 41.8781
          TF_VAR_chicago_longitude: -87.6298

  ingest-historical-weather:
    name: Ingest Historical Weather Data
    runs-on: ubuntu-latest
    needs: deploy-terraform
    defaults:
      run:
        working-directory: ./
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install google-cloud-bigquery requests
      
      - name: Setup GCP Auth
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Setup GCP Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
      
      - name: Get Cloud Function URL
        id: get-function-url
        run: |
          FUNCTION_URL=$(gcloud functions describe weather-ingestion \
            --region=${{ secrets.GCP_REGION || 'us-central1' }} \
            --gen2 \
            --format="value(serviceConfig.uri)" \
            --project=${{ secrets.GCP_PROJECT_ID }})
          echo "url=$FUNCTION_URL" >> $GITHUB_OUTPUT
        continue-on-error: false
      
      - name: Check if historical data exists
        id: check-historical
        run: |
          python3 << 'PYEOF'
          from google.cloud import bigquery
          import os
          
          project_id = os.environ['GCP_PROJECT_ID']
          client = bigquery.Client(project=project_id)
          
          # Verificar si ya existen datos hist√≥ricos (junio-diciembre 2023)
          query = f"""
          SELECT COUNT(*) as count 
          FROM `{project_id}.chicago_taxi_raw.weather_data`
          WHERE date >= '2023-06-01' AND date <= '2023-12-31'
          """
          
          try:
              result = client.query(query).result()
              count = next(result).count
              
              print(f"üìä Datos hist√≥ricos encontrados: {count} d√≠as")
              
              # Si hay al menos 180 d√≠as (6 meses), considerar que los datos hist√≥ricos ya existen
              if count >= 180:
                  print("‚úÖ Datos hist√≥ricos ya existen. Saltando ingesta hist√≥rica.")
                  with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                      f.write("needs_historical=false\n")
              else:
                  print(f"‚ö†Ô∏è  Solo hay {count} d√≠as. Necesitamos ingesta hist√≥rica.")
                  with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                      f.write("needs_historical=true\n")
          except Exception as e:
              print(f"‚ö†Ô∏è  Error verificando datos: {e}. Asumiendo que necesitamos ingesta hist√≥rica.")
              with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                  f.write("needs_historical=true\n")
          PYEOF
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GITHUB_OUTPUT: ${{ github.output }}
      
      - name: Trigger Historical Ingestion
        if: steps.check-historical.outputs.needs_historical == 'true'
        run: |
          FUNCTION_URL="${{ steps.get-function-url.outputs.url }}"
          echo "üîÑ Triggering historical ingestion at: $FUNCTION_URL"
          
          TOKEN=$(gcloud auth print-identity-token)
          
          curl -X POST "$FUNCTION_URL" \
            -H "Authorization: Bearer $TOKEN" \
            -H "Content-Type: application/json" \
            -d '{"historical": true}' \
            --max-time 900 \
            --fail-with-body \
            --show-error
          
          echo "‚úÖ Historical ingestion completed"
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
      
      - name: Skip Historical Ingestion
        if: steps.check-historical.outputs.needs_historical == 'false'
        run: |
          echo "‚è≠Ô∏è  Datos hist√≥ricos ya existen. Saltando ingesta hist√≥rica."

  setup-airflow:
    name: Setup Airflow (Cloud Composer)
    runs-on: ubuntu-latest
    needs: deploy-terraform
    defaults:
      run:
        working-directory: ./
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 2  # Necesario para detectar cambios
      
      - name: Detect changes in Airflow files
        id: changes
        uses: dorny/paths-filter@v2
        with:
          filters: |
            airflow:
              - 'airflow/**'
            dbt:
              - 'dbt/**'
            airflow_config:
              - '.github/workflows/cd.yml'
      
      - name: Setup GCP Auth
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Setup GCP Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
      
      - name: Enable Composer API
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          echo "üîß Habilitando Cloud Composer API..."
          gcloud services enable composer.googleapis.com --project="$PROJECT_ID" || echo "‚ö†Ô∏è  API ya habilitada o error"
          echo "‚è≥ Esperando propagaci√≥n de API (10 segundos)..."
          sleep 10
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        continue-on-error: true
      
      - name: Grant Composer permissions to service account
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          SA_EMAIL=$(echo '${{ secrets.GCP_SA_KEY }}' | jq -r '.client_email')
          
          echo "üîê Otorgando permisos de Composer al service account..."
          gcloud projects add-iam-policy-binding "$PROJECT_ID" \
            --member="serviceAccount:$SA_EMAIL" \
            --role="roles/composer.admin" \
            --condition=None 2>&1 || echo "‚ö†Ô∏è  Error otorgando permisos (puede que ya existan)"
          
          echo "‚è≥ Esperando propagaci√≥n de permisos (5 segundos)..."
          sleep 5
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        continue-on-error: true
      
      - name: Grant permissions to Cloud Composer Service Agent
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          
          # Obtener el PROJECT_NUMBER del proyecto
          echo "üîç Obteniendo PROJECT_NUMBER del proyecto..."
          PROJECT_NUMBER=$(gcloud projects describe "$PROJECT_ID" --format="value(projectNumber)" 2>/dev/null || echo "")
          
          if [ -z "$PROJECT_NUMBER" ]; then
            echo "‚ùå No se pudo obtener PROJECT_NUMBER. El service agent no se puede configurar autom√°ticamente."
            echo "   Por favor, otorga manualmente el rol roles/composer.ServiceAgentV2Ext al service agent:"
            echo "   service-{PROJECT_NUMBER}@cloudcomposer-accounts.iam.gserviceaccount.com"
            echo "   Puedes encontrar el PROJECT_NUMBER en: https://console.cloud.google.com/iam-admin/settings?project=$PROJECT_ID"
            exit 1
          fi
          
          COMPOSER_SERVICE_AGENT="service-${PROJECT_NUMBER}@cloudcomposer-accounts.iam.gserviceaccount.com"
          echo "üîê Otorgando permisos al Cloud Composer Service Agent: $COMPOSER_SERVICE_AGENT"
          echo "   Rol: roles/composer.ServiceAgentV2Ext"
          
          gcloud projects add-iam-policy-binding "$PROJECT_ID" \
            --member="serviceAccount:$COMPOSER_SERVICE_AGENT" \
            --role="roles/composer.ServiceAgentV2Ext" \
            --condition=None 2>&1 || echo "‚ö†Ô∏è  Error otorgando permisos (puede que ya existan)"
          
          echo "‚è≥ Esperando propagaci√≥n de permisos (5 segundos)..."
          sleep 5
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        continue-on-error: false
      
      - name: Check if Composer environment exists
        id: check-composer
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          echo "üîç Verificando si existe el entorno de Composer..."
          
          # Verificar si existe (puede estar en cualquier estado: RUNNING, CREATING, UPDATING, etc.)
          ENV_INFO=$(gcloud composer environments describe "$COMPOSER_ENV" \
            --location "$REGION" \
            --project "$PROJECT_ID" \
            --format="value(name,state)" 2>/dev/null || echo "")
          
          if [ -n "$ENV_INFO" ]; then
            ENV_STATE=$(echo "$ENV_INFO" | awk '{print $NF}')
            echo "‚úÖ Composer environment existe (estado: $ENV_STATE)"
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "state=$ENV_STATE" >> $GITHUB_OUTPUT
            
            # Si est√° cre√°ndose, no intentar crearlo de nuevo
            if [ "$ENV_STATE" == "CREATING" ] || [ "$ENV_STATE" == "UPDATING" ]; then
              echo "‚ö†Ô∏è  Composer est√° en proceso de creaci√≥n/actualizaci√≥n. No se intentar√° crear de nuevo."
              echo "creating=true" >> $GITHUB_OUTPUT
            else
              echo "creating=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "‚ö†Ô∏è  Composer environment no existe"
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "creating=false" >> $GITHUB_OUTPUT
          fi
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
        continue-on-error: false
      
      - name: Create Composer environment (if needed)
        id: create-composer
        if: steps.check-composer.outputs.exists != 'true' && steps.check-composer.outputs.creating != 'true'
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          # Obtener el email del service account de GitHub Actions
          SA_EMAIL=$(echo '${{ secrets.GCP_SA_KEY }}' | jq -r '.client_email')
          
          echo "üîÑ Creando entorno de Cloud Composer..."
          echo "‚ö†Ô∏è  NOTA: Esto puede tardar 20-30 minutos."
          echo "   El workflow NO esperar√° a que termine - Composer se crear√° en background."
          echo "   Puedes verificar el progreso en:"
          echo "   https://console.cloud.google.com/composer/environments?project=$PROJECT_ID"
          echo ""
          echo "üîí IMPORTANTE: Si este workflow se ejecuta de nuevo mientras Composer se est√° creando,"
          echo "   NO intentar√° crearlo de nuevo (se detectar√° que est√° en estado CREATING)."
          echo ""
          echo "üìã Service Account para Composer: $SA_EMAIL"
          
          # Verificar una vez m√°s antes de crear (por si acaso cambi√≥ entre el check y ahora)
          ENV_CHECK=$(gcloud composer environments describe "$COMPOSER_ENV" \
            --location "$REGION" \
            --project "$PROJECT_ID" \
            --format="value(name)" 2>/dev/null || echo "")
          
          if [ -n "$ENV_CHECK" ]; then
            echo "‚úÖ Composer ya existe (fue creado entre el check y ahora). Saltando creaci√≥n."
            echo "created=false" >> $GITHUB_OUTPUT
            echo "status=exists" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Intentar crear (puede fallar si ya existe o por permisos)
          # NOTA: Composer 2.x requiere --service-account expl√≠cito
          # NOTA: Composer 2.x no acepta --machine-type, --disk-size, --python-version, ni --node-count
          # Usamos --async para que no espere (evita timeout del workflow)
          echo "üîÑ Ejecutando comando de creaci√≥n..."
          
          # Ejecutar el comando y capturar output y exit code
          # Usar temporal file para asegurar que capturamos todo el output
          TEMP_OUTPUT=$(mktemp)
          set +e  # Desactivar error estricto temporalmente
          gcloud composer environments create "$COMPOSER_ENV" \
            --location "$REGION" \
            --project "$PROJECT_ID" \
            --image-version composer-2.16.3-airflow-2.10.5 \
            --service-account "$SA_EMAIL" \
            --async > "$TEMP_OUTPUT" 2>&1
          CREATE_EXIT=$?
          CREATE_OUTPUT=$(cat "$TEMP_OUTPUT")
          rm -f "$TEMP_OUTPUT"
          set -e  # Reactivar error estricto
          
          # SIEMPRE mostrar el output para debugging
          echo ""
          echo "üìã Output del comando gcloud composer create:"
          echo "----------------------------------------"
          echo "$CREATE_OUTPUT"
          echo "----------------------------------------"
          echo "Exit code: $CREATE_EXIT"
          echo ""
          
          if [ $CREATE_EXIT -eq 0 ]; then
            echo "‚úÖ Comando de creaci√≥n de Composer iniciado (ejecut√°ndose en background)"
            echo "created=true" >> $GITHUB_OUTPUT
            echo "status=creating" >> $GITHUB_OUTPUT
          else
            # Analizar el error
            if echo "$CREATE_OUTPUT" | grep -qi "already exists\|Already exists"; then
              echo "‚úÖ Composer ya existe. Continuando..."
              echo "created=false" >> $GITHUB_OUTPUT
              echo "status=exists" >> $GITHUB_OUTPUT
            else
              echo ""
              echo "=========================================="
              echo "‚ùå ERROR CR√çTICO: No se pudo crear Composer"
              echo "=========================================="
              echo ""
              echo "Detalles del error:"
              echo "$CREATE_OUTPUT"
              echo ""
              echo "Posibles causas:"
              echo "  1. Falta el rol roles/composer.admin en el service account"
              echo "  2. La API composer.googleapis.com no est√° habilitada"
              echo "  3. Cuota insuficiente en el proyecto"
              echo "  4. Error de permisos o configuraci√≥n"
              echo ""
              echo "üîç Verifica en la consola:"
              echo "   https://console.cloud.google.com/composer/environments?project=$PROJECT_ID"
              echo ""
              echo "üîß Soluci√≥n:"
              echo "   1. Verifica los permisos del service account"
              echo "   2. Habilita la API: gcloud services enable composer.googleapis.com"
              echo "   3. Revisa las cuotas del proyecto"
              echo ""
              echo "=========================================="
              echo "created=false" >> $GITHUB_OUTPUT
              echo "status=error" >> $GITHUB_OUTPUT
              exit 1  # FALLAR el workflow para que se notifique
            fi
          fi
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
        continue-on-error: false
      
      - name: Get Composer bucket (with retries)
        id: get-bucket
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          # Si hubo error al crear, verificar el estado antes de intentar obtener el bucket
          if [ "${{ steps.create-composer.outputs.status }}" == "error" ]; then
            echo ""
            echo "=========================================="
            echo "‚ùå ERROR: Composer no se pudo crear"
            echo "=========================================="
            echo ""
            echo "El workflow fallar√° para notificar el error."
            echo "Verifica los logs del step anterior para m√°s detalles."
            echo ""
            exit 1
          fi
          
          echo "üîç Obteniendo bucket de Composer (con reintentos)..."
          
          # Si se acaba de crear, esperar un poco antes de intentar obtener el bucket
          if [ "${{ steps.create-composer.outputs.status }}" == "creating" ]; then
            echo "‚è≥ Composer se est√° creando. Esperando 60 segundos antes de intentar obtener el bucket..."
            sleep 60
          fi
          
          # Intentar hasta 6 veces (3 minutos total)
          for i in {1..6}; do
            echo "   Intento $i/6..."
            
            # Verificar el estado de Composer
            ENV_STATE=$(gcloud composer environments describe "$COMPOSER_ENV" \
              --location "$REGION" \
              --project "$PROJECT_ID" \
              --format="value(state)" 2>/dev/null || echo "NOT_FOUND")
            
            if [ "$ENV_STATE" == "NOT_FOUND" ]; then
              echo "   ‚ö†Ô∏è  Composer no encontrado. Puede estar a√∫n cre√°ndose o hubo un error."
            elif [ "$ENV_STATE" == "CREATING" ] || [ "$ENV_STATE" == "UPDATING" ]; then
              echo "   ‚è≥ Composer est√° en estado: $ENV_STATE (a√∫n no est√° listo)"
            elif [ "$ENV_STATE" == "RUNNING" ]; then
              echo "   ‚úÖ Composer est√° RUNNING. Obteniendo bucket..."
            elif [ "$ENV_STATE" == "ERROR" ] || [ "$ENV_STATE" == "FAILED" ]; then
              echo ""
              echo "=========================================="
              echo "‚ùå ERROR: Composer fall√≥ al crearse"
              echo "=========================================="
              echo ""
              echo "Estado: $ENV_STATE"
              echo ""
              echo "üîç Verifica en la consola:"
              echo "   https://console.cloud.google.com/composer/environments?project=$PROJECT_ID"
              echo ""
              echo "El workflow fallar√° para notificar el error."
              echo ""
              exit 1
            else
              echo "   ‚ÑπÔ∏è  Estado de Composer: $ENV_STATE"
            fi
            
            BUCKET=$(gcloud composer environments describe "$COMPOSER_ENV" \
              --location "$REGION" \
              --project "$PROJECT_ID" \
              --format="value(config.dagGcsPrefix)" 2>/dev/null | sed 's|/dags||' || echo "")
            
            if [ -n "$BUCKET" ]; then
              echo "‚úÖ Bucket encontrado: $BUCKET"
              echo "bucket=$BUCKET" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            if [ $i -lt 6 ]; then
              echo "   Esperando 30 segundos antes del siguiente intento..."
              sleep 30
            fi
          done
          
          echo ""
          echo "=========================================="
          echo "‚ö†Ô∏è  ADVERTENCIA: No se pudo obtener el bucket de Composer"
          echo "=========================================="
          echo ""
          echo "Esto puede significar:"
          echo "  1. Composer a√∫n se est√° creando (tarda 20-30 minutos)"
          echo "  2. Hubo un error en la creaci√≥n (verifica en la consola)"
          echo ""
          echo "üìã Pr√≥ximos pasos:"
          echo "1. Verifica el estado en:"
          echo "   https://console.cloud.google.com/composer/environments?project=$PROJECT_ID"
          echo ""
          echo "2. Si Composer est√° CREATING, espera 20-30 minutos y vuelve a ejecutar el workflow"
          echo ""
          echo "3. Si Composer est√° en ERROR, revisa los logs en la consola"
          echo ""
          echo "‚ö†Ô∏è  Continuando sin subir DAGs (se pueden subir despu√©s manualmente)"
          echo "bucket=" >> $GITHUB_OUTPUT
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
        continue-on-error: true
      
      - name: Determine if uploads are needed
        id: should-upload
        run: |
          # Subir archivos SIEMPRE cuando:
          # 1. Composer est√° listo (bucket disponible), Y
          # 2. Es ejecuci√≥n manual, O hay cambios, O Composer existe (para asegurar que los DAGs est√©n)
          COMPOSER_EXISTS="${{ steps.check-composer.outputs.exists == 'true' }}"
          AIRFLOW_CHANGED="${{ steps.changes.outputs.airflow == 'true' }}"
          DBT_CHANGED="${{ steps.changes.outputs.dbt == 'true' }}"
          CONFIG_CHANGED="${{ steps.changes.outputs.airflow_config == 'true' }}"
          IS_MANUAL="${{ github.event_name == 'workflow_dispatch' }}"
          
          echo "üîç Evaluando si se deben subir archivos:"
          echo "   - Composer existe: $COMPOSER_EXISTS"
          echo "   - Cambios en Airflow: $AIRFLOW_CHANGED"
          echo "   - Cambios en dbt: $DBT_CHANGED"
          echo "   - Cambios en config: $CONFIG_CHANGED"
          echo "   - Ejecuci√≥n manual: $IS_MANUAL"
          echo ""
          
          # Si es ejecuci√≥n manual, SIEMPRE subir
          if [ "$IS_MANUAL" == "true" ]; then
            echo "‚úÖ Ejecuci√≥n manual - se actualizar√°n los archivos"
            echo "upload=true" >> $GITHUB_OUTPUT
          # Si Composer existe, SIEMPRE subir (para asegurar que los DAGs est√©n presentes)
          elif [ "$COMPOSER_EXISTS" == "true" ]; then
            echo "‚úÖ Composer existe - subiendo archivos para asegurar que los DAGs est√©n presentes"
            echo "upload=true" >> $GITHUB_OUTPUT
          # Si hay cambios, subir
          elif [ "$AIRFLOW_CHANGED" == "true" ] || \
               [ "$DBT_CHANGED" == "true" ] || \
               [ "$CONFIG_CHANGED" == "true" ]; then
            echo "‚úÖ Cambios detectados - se actualizar√°n los archivos"
            echo "upload=true" >> $GITHUB_OUTPUT
          else
            echo "‚è≠Ô∏è  No se cumplen condiciones para subir archivos"
            echo "upload=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Upload DAGs to Composer
        if: steps.get-bucket.outputs.bucket != '' && steps.should-upload.outputs.upload == 'true'
        run: |
          BUCKET_RAW="${{ steps.get-bucket.outputs.bucket }}"
          
          # Normalizar el bucket: remover gs:// si est√° presente y luego agregarlo
          BUCKET=$(echo "$BUCKET_RAW" | sed 's|^gs://||')
          BUCKET_PATH="gs://$BUCKET"
          
          echo "üì§ Subiendo DAGs a Composer..."
          echo "   Bucket raw: $BUCKET_RAW"
          echo "   Bucket normalizado: $BUCKET"
          echo "   Destino: $BUCKET_PATH/dags/"
          echo ""
          
          # Verificar que los archivos existen
          if [ ! -d "airflow/dags" ]; then
            echo "‚ùå ERROR: Directorio airflow/dags no existe"
            exit 1
          fi
          
          # Listar archivos que se van a subir
          echo "üìã Archivos DAG encontrados:"
          ls -la airflow/dags/*.py || echo "‚ö†Ô∏è  No se encontraron archivos .py"
          echo ""
          
          # Subir cada archivo .py individualmente para mejor control
          for dag_file in airflow/dags/*.py; do
            if [ -f "$dag_file" ]; then
              filename=$(basename "$dag_file")
              echo "üì§ Subiendo $filename..."
              gsutil cp "$dag_file" "$BUCKET_PATH/dags/$filename"
              if [ $? -eq 0 ]; then
                echo "   ‚úÖ $filename subido correctamente"
              else
                echo "   ‚ùå Error subiendo $filename"
                exit 1
              fi
            fi
          done
          
          # Subir requirements.txt si existe
          if [ -f "airflow/dags/requirements.txt" ]; then
            echo "üì§ Subiendo requirements.txt..."
            gsutil cp airflow/dags/requirements.txt "$BUCKET_PATH/data/requirements.txt"
            echo "   ‚úÖ requirements.txt subido"
          fi
          
          echo ""
          echo "‚úÖ Todos los DAGs subidos correctamente"
          
          # Verificar que se subieron
          echo ""
          echo "üîç Verificando DAGs en el bucket:"
          gsutil ls "$BUCKET_PATH/dags/*.py" || echo "‚ö†Ô∏è  No se encontraron DAGs en el bucket"
        continue-on-error: false
      
      - name: Upload dbt to Composer
        if: steps.get-bucket.outputs.bucket != '' && steps.should-upload.outputs.upload == 'true'
        run: |
          BUCKET_RAW="${{ steps.get-bucket.outputs.bucket }}"
          # Normalizar el bucket: remover gs:// si est√° presente y luego agregarlo
          BUCKET=$(echo "$BUCKET_RAW" | sed 's|^gs://||')
          BUCKET_PATH="gs://$BUCKET"
          echo "üì§ Subiendo dbt a Composer..."
          gsutil -m cp -r dbt/* "$BUCKET_PATH/data/dbt/" || true
          echo "‚úÖ dbt subido"
        continue-on-error: true
      
      - name: Upload service account key to Composer
        if: steps.get-bucket.outputs.bucket != '' && steps.should-upload.outputs.upload == 'true'
        run: |
          BUCKET_RAW="${{ steps.get-bucket.outputs.bucket }}"
          # Normalizar el bucket: remover gs:// si est√° presente y luego agregarlo
          BUCKET=$(echo "$BUCKET_RAW" | sed 's|^gs://||')
          BUCKET_PATH="gs://$BUCKET"
          if [ -f "github-actions-key.json" ]; then
            echo "üì§ Subiendo service account key..."
            gsutil cp github-actions-key.json "$BUCKET_PATH/data/github-actions-key.json" || true
            echo "‚úÖ Service account key subido"
          else
            echo "‚ö†Ô∏è  github-actions-key.json no encontrado. Usando credenciales por defecto."
          fi
        continue-on-error: true
      
      - name: Skip uploads (no changes or Composer not ready)
        if: steps.get-bucket.outputs.bucket == '' || steps.should-upload.outputs.upload != 'true'
        run: |
          if [ "${{ steps.get-bucket.outputs.bucket }}" == "" ]; then
            echo "‚è≠Ô∏è  Saltando subida de archivos - Composer a√∫n no est√° listo"
            echo "   El entorno puede estar a√∫n cre√°ndose (tarda 20-30 minutos)"
          else
            echo "‚è≠Ô∏è  Saltando subida de archivos - No hay cambios en archivos de Airflow"
            echo "   Los archivos solo se actualizan cuando hay cambios en:"
            echo "   - airflow/**"
            echo "   - dbt/**"
            echo "   - .github/workflows/cd.yml"
          fi
      
      - name: Set Airflow variables
        if: steps.get-bucket.outputs.bucket != '' && (steps.should-upload.outputs.upload == 'true' || steps.check-composer.outputs.exists != 'true')
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          echo "üîß Configurando variables de Airflow..."
          
          # Configurar variables usando gcloud composer
          gcloud composer environments run "$COMPOSER_ENV" \
            --location "$REGION" \
            variables -- \
            --set GCP_PROJECT_ID "$PROJECT_ID" \
            --set GCP_REGION "$REGION" \
            --set DBT_PROJECT_DIR "/home/airflow/gcs/data/dbt" \
            --set DBT_PROFILES_DIR "/home/airflow/gcs/data/dbt" \
            --set DBT_PROFILE "chicago_taxi_analysis" \
            --set GCP_SA_KEY_PATH "/home/airflow/gcs/data/github-actions-key.json" \
            --set OPENWEATHER_API_KEY "${{ secrets.OPENWEATHER_API_KEY || '' }}" || echo "‚ö†Ô∏è  Error configurando variables (puede requerir Airflow CLI)"
          
          echo "‚úÖ Variables configuradas"
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
        continue-on-error: true
      
      - name: Airflow setup summary
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          echo ""
          echo "=========================================="
          echo "‚úÖ AIRFLOW CONFIGURADO"
          echo "=========================================="
          echo ""
          echo "üìã Pr√≥ximos pasos:"
          echo "1. Ve a Airflow UI:"
          echo "   https://console.cloud.google.com/composer/environments/$COMPOSER_ENV/monitoring?project=$PROJECT_ID"
          echo ""
          echo "2. Trigger el DAG 'chicago_taxi_historical_ingestion' (una vez)"
          echo ""
          echo "3. El DAG 'chicago_taxi_daily_pipeline' se ejecutar√° autom√°ticamente todos los d√≠as a las 2 AM UTC"
          echo ""
          echo "=========================================="
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
