name: CD Pipeline - Deploy Infrastructure

on:
  push:
    branches: [ main ]
    paths:
      - 'terraform/**'
      - 'functions/**'
      - 'airflow/**'
      - 'dbt/**'
      - '.github/workflows/cd.yml'
  workflow_dispatch:  # Permite ejecuci√≥n manual

env:
  TF_VERSION: 1.5.0
  PYTHON_VERSION: '3.9'

jobs:
  detect-changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    outputs:
      terraform: ${{ steps.filter.outputs.terraform }}
      functions: ${{ steps.filter.outputs.functions }}
      airflow_dags: ${{ steps.filter.outputs.airflow_dags }}
      dbt: ${{ steps.filter.outputs.dbt }}
      airflow_config: ${{ steps.filter.outputs.airflow_config }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Detect file changes
        id: filter
        uses: dorny/paths-filter@v2
        with:
          filters: |
            terraform:
              - 'terraform/**'
            functions:
              - 'functions/**'
            airflow_dags:
              - 'airflow/dags/**'
            dbt:
              - 'dbt/**'
            airflow_config:
              - '.github/workflows/cd.yml'
  
  prepare-function-zip:
    name: Prepare Cloud Function ZIP
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.terraform == 'true' || needs.detect-changes.outputs.functions == 'true'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Create function ZIP
        run: |
          cd functions/weather_ingestion
          zip -r ../../terraform/weather-ingestion-source.zip . \
            -x "*.pyc" "__pycache__/*" "*.git*" "*.zip"
      
      - name: Upload ZIP artifact
        uses: actions/upload-artifact@v4
        with:
          name: function-zip
          path: terraform/weather-ingestion-source.zip
          retention-days: 1

  deploy-terraform:
    name: Deploy Infrastructure with Terraform
    runs-on: ubuntu-latest
    needs: [detect-changes, prepare-function-zip]
    if: needs.detect-changes.outputs.terraform == 'true' || needs.detect-changes.outputs.functions == 'true'
    defaults:
      run:
        working-directory: ./terraform
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Download function ZIP
        uses: actions/download-artifact@v4
        with:
          name: function-zip
          path: terraform/
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false
      
      - name: Setup GCP Auth
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Setup GCP Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
      
      - name: Enable Required APIs
        run: |
          gcloud services enable \
            bigquery.googleapis.com \
            cloudfunctions.googleapis.com \
            cloudscheduler.googleapis.com \
            storage.googleapis.com \
            datacatalog.googleapis.com \
            cloudbuild.googleapis.com \
            run.googleapis.com \
            --project=${{ secrets.GCP_PROJECT_ID }}
      
      - name: Create Terraform State Bucket
        run: |
          STATE_BUCKET="${{ secrets.GCP_PROJECT_ID }}-terraform-state"
          if ! gsutil ls -b gs://$STATE_BUCKET 2>/dev/null; then
            echo "üì¶ Creando bucket para estado de Terraform..."
            gsutil mb -p ${{ secrets.GCP_PROJECT_ID }} -l ${{ secrets.GCP_REGION || 'us-central1' }} gs://$STATE_BUCKET
            gsutil versioning set on gs://$STATE_BUCKET
            echo "‚úÖ Bucket creado: $STATE_BUCKET"
          else
            echo "‚úÖ Bucket ya existe: $STATE_BUCKET"
          fi
      
      - name: Terraform Init
        run: |
          STATE_BUCKET="${{ secrets.GCP_PROJECT_ID }}-terraform-state"
          terraform init \
            -backend-config="bucket=$STATE_BUCKET" \
            -backend-config="prefix=terraform/state" \
            -reconfigure
      
      - name: Force Unlock Terraform State (if locked)
        run: |
          echo "üîì Verificando si hay locks en el estado de Terraform..."
          STATE_BUCKET="${{ secrets.GCP_PROJECT_ID }}-terraform-state"
          LOCK_FILE="gs://${STATE_BUCKET}/terraform/state/default.tflock"
          
          # Verificar si el archivo de lock existe
          if gsutil stat "$LOCK_FILE" &>/dev/null; then
            echo "‚ö†Ô∏è  Lock detectado. Intentando leer informaci√≥n del lock..."
            LOCK_CONTENT=$(gsutil cat "$LOCK_FILE" 2>/dev/null || echo "")
            
            # Extraer el Lock ID del contenido
            LOCK_ID=$(echo "$LOCK_CONTENT" | grep -E "^\s*ID:\s*[0-9]+" | sed -E 's/^\s*ID:\s*([0-9]+).*/\1/' | head -1 || echo "")
            
            if [ -n "$LOCK_ID" ]; then
              echo "üîì Lock ID encontrado: $LOCK_ID. Forzando unlock..."
              terraform force-unlock -force "$LOCK_ID" || {
                echo "‚ö†Ô∏è  No se pudo forzar unlock con terraform. Intentando eliminar el lock directamente..."
                # Como √∫ltimo recurso, eliminar el archivo de lock directamente
                gsutil rm "$LOCK_FILE" 2>/dev/null && echo "‚úÖ Lock eliminado directamente" || echo "‚ö†Ô∏è  No se pudo eliminar el lock"
              }
            else
              echo "‚ö†Ô∏è  Lock detectado pero no se pudo extraer el ID. Eliminando lock directamente..."
              gsutil rm "$LOCK_FILE" 2>/dev/null && echo "‚úÖ Lock eliminado" || echo "‚ö†Ô∏è  No se pudo eliminar el lock"
            fi
          else
            echo "‚úÖ No hay locks activos"
          fi
        continue-on-error: true
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
      
      - name: Import existing resources
        run: |
          set +e  # Desactivar modo de error estricto para este step
          echo "üîÑ Verificando y importando recursos existentes..."
          
          # Funci√≥n para importar si el recurso existe
          import_if_exists() {
            local resource=$1
            local import_id=$2
            local resource_name=$3
            
            # Verificar si el recurso ya est√° en el estado
            if terraform state show "$resource" &>/dev/null; then
              echo "‚úì $resource_name ya est√° en el estado"
              return 0
            fi
            
            # Intentar importar
            echo "üîÑ Intentando importar $resource_name..."
            IMPORT_OUTPUT=$(terraform import "$resource" "$import_id" 2>&1)
            IMPORT_EXIT=$?
            if [ $IMPORT_EXIT -eq 0 ]; then
              echo "‚úÖ Importado: $resource_name"
              return 0
            else
              # Si el error es que ya existe, intentar forzar el import
              if echo "$IMPORT_OUTPUT" | grep -q "already exists\|Already Exists\|Resource already managed"; then
                echo "‚ö†Ô∏è  $resource_name ya existe. Intentando forzar import..."
                # El recurso existe pero no est√° en el estado, intentar importar de nuevo con -lock=false
                terraform import -lock=false "$resource" "$import_id" 2>&1 || echo "   No se pudo forzar import"
              else
                echo "‚ö†Ô∏è  No se pudo importar $resource_name (esto es OK si el recurso no existe a√∫n):"
                echo "   $IMPORT_OUTPUT"
              fi
              return 0  # No fallar
            fi
          }
          
          # Importar datasets de BigQuery
          import_if_exists "google_bigquery_dataset.raw_dataset" "${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_raw" "Dataset raw"
          import_if_exists "google_bigquery_dataset.silver_dataset" "${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_silver" "Dataset silver"
          import_if_exists "google_bigquery_dataset.gold_dataset" "${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_gold" "Dataset gold"
          
          # Importar service account
          import_if_exists "google_service_account.weather_ingestion_sa" "projects/${{ secrets.GCP_PROJECT_ID }}/serviceAccounts/weather-ingestion-sa@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com" "Service account"
          
          # Importar storage bucket
          import_if_exists "google_storage_bucket.function_source" "${{ secrets.GCP_PROJECT_ID }}-function-source" "Storage bucket"
          
          # Intentar importar taxonomy (m√°s complejo)
          TAXONOMY_NAME=$(gcloud data-catalog taxonomies list --location=${{ secrets.GCP_REGION || 'us-central1' }} --format='value(name)' --project=${{ secrets.GCP_PROJECT_ID }} 2>/dev/null | head -1 | xargs basename 2>/dev/null || echo "")
          if [ -n "$TAXONOMY_NAME" ]; then
            import_if_exists "google_data_catalog_taxonomy.sensitive_data" "projects/${{ secrets.GCP_PROJECT_ID }}/locations/${{ secrets.GCP_REGION || 'us-central1' }}/taxonomies/$TAXONOMY_NAME" "Data Catalog Taxonomy"
          else
            echo "‚ö†Ô∏è  Taxonomy no encontrada (se crear√° si es necesario)"
          fi
          
          echo "‚úÖ Proceso de import completado"
          echo "üìä Estado actual:"
          terraform state list || echo "Estado vac√≠o o no disponible"
          echo "üíæ Sincronizando estado con backend remoto..."
          terraform force-unlock -force $(terraform state list | wc -l) 2>/dev/null || true
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_developer_email: ${{ secrets.DEVELOPER_EMAIL }}
          TF_VAR_openweather_api_key: ${{ secrets.OPENWEATHER_API_KEY || '' }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
          TF_VAR_chicago_latitude: 41.8781
          TF_VAR_chicago_longitude: -87.6298
        continue-on-error: true
      
      - name: Terraform Plan
        run: terraform plan -out=tfplan -lock-timeout=5m
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_developer_email: ${{ secrets.DEVELOPER_EMAIL }}
          TF_VAR_openweather_api_key: ${{ secrets.OPENWEATHER_API_KEY || '' }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
          TF_VAR_chicago_latitude: 41.8781
          TF_VAR_chicago_longitude: -87.6298
        continue-on-error: false
      
      - name: Terraform Apply
        run: |
          echo "üîÑ Aplicando cambios de Terraform..."
          # Intentar aplicar el plan con timeout para el lock
          if terraform apply -auto-approve -lock-timeout=5m tfplan 2>&1 | tee /tmp/apply_output.log; then
            echo "‚úÖ Terraform apply exitoso"
            exit 0
          else
            APPLY_EXIT=$?
            APPLY_OUTPUT=$(cat /tmp/apply_output.log)
            
            # Si el error es por recursos existentes, intentar importarlos y re-aplicar
            if echo "$APPLY_OUTPUT" | grep -q "Already Exists\|already exists\|Error 409"; then
              echo "‚ö†Ô∏è  Detectados recursos existentes. Intentando importar y re-aplicar..."
              
              # Re-importar recursos que fallaron
              terraform import google_bigquery_dataset.raw_dataset ${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_raw 2>/dev/null || true
              terraform import google_bigquery_dataset.silver_dataset ${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_silver 2>/dev/null || true
              terraform import google_bigquery_dataset.gold_dataset ${{ secrets.GCP_PROJECT_ID }}:chicago_taxi_gold 2>/dev/null || true
              terraform import google_service_account.weather_ingestion_sa projects/${{ secrets.GCP_PROJECT_ID }}/serviceAccounts/weather-ingestion-sa@${{ secrets.GCP_PROJECT_ID }}.iam.gserviceaccount.com 2>/dev/null || true
              terraform import google_storage_bucket.function_source ${{ secrets.GCP_PROJECT_ID }}-function-source 2>/dev/null || true
              
              # Re-aplicar sin el plan (para que use el estado actualizado)
              echo "üîÑ Re-aplicando con estado actualizado..."
              terraform apply -auto-approve -lock-timeout=5m || {
                echo "‚ö†Ô∏è  Re-apply tambi√©n fall√≥, pero algunos recursos pueden haberse creado"
                exit 0  # No fallar completamente
              }
            else
              echo "‚ùå Terraform apply fall√≥ con error diferente:"
              echo "$APPLY_OUTPUT"
              exit $APPLY_EXIT
            fi
          fi
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_developer_email: ${{ secrets.DEVELOPER_EMAIL }}
          TF_VAR_openweather_api_key: ${{ secrets.OPENWEATHER_API_KEY || '' }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
          TF_VAR_chicago_latitude: 41.8781
          TF_VAR_chicago_longitude: -87.6298
        continue-on-error: false
      
      - name: Terraform Output
        run: terraform output
        env:
          TF_VAR_project_id: ${{ secrets.GCP_PROJECT_ID }}
          TF_VAR_developer_email: ${{ secrets.DEVELOPER_EMAIL }}
          TF_VAR_openweather_api_key: ${{ secrets.OPENWEATHER_API_KEY || '' }}
          TF_VAR_region: ${{ secrets.GCP_REGION || 'us-central1' }}
          TF_VAR_chicago_latitude: 41.8781
          TF_VAR_chicago_longitude: -87.6298

  quick-update-dags:
    name: Quick Update DAGs Only
    runs-on: ubuntu-latest
    needs: detect-changes
    if: |
      (needs.detect-changes.outputs.airflow_dags == 'true' || needs.detect-changes.outputs.dbt == 'true') &&
      needs.detect-changes.outputs.terraform != 'true' &&
      needs.detect-changes.outputs.functions != 'true'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Setup GCP Auth
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Setup GCP Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
      
      - name: Check if Composer exists
        id: check-composer
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          ENV_STATE=$(gcloud composer environments describe "$COMPOSER_ENV" \
            --location "$REGION" \
            --project "$PROJECT_ID" \
            --format="value(state)" 2>/dev/null || echo "NOT_FOUND")
          
          if [ "$ENV_STATE" == "RUNNING" ]; then
            echo "‚úÖ Composer existe y est√° RUNNING"
            echo "exists=true" >> $GITHUB_OUTPUT
          elif [ "$ENV_STATE" == "CREATING" ] || [ "$ENV_STATE" == "UPDATING" ]; then
            echo "‚è≥ Composer est√° en estado: $ENV_STATE"
            echo "exists=false" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "‚ùå Composer no existe o est√° en estado: $ENV_STATE"
            echo "exists=false" >> $GITHUB_OUTPUT
            exit 1
          fi
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
        continue-on-error: true
      
      - name: Get Composer bucket
        if: steps.check-composer.outputs.exists == 'true'
        id: get-bucket
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          BUCKET=$(gcloud composer environments describe "$COMPOSER_ENV" \
            --location "$REGION" \
            --project "$PROJECT_ID" \
            --format="value(config.dagGcsPrefix)" 2>/dev/null | sed 's|/dags||' || echo "")
          
          if [ -z "$BUCKET" ]; then
            echo "‚ùå No se pudo obtener el bucket de Composer"
            exit 1
          fi
          
          echo "‚úÖ Bucket: $BUCKET"
          echo "bucket=$BUCKET" >> $GITHUB_OUTPUT
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
        continue-on-error: false
      
      - name: Upload DAGs to Composer
        if: steps.get-bucket.outputs.bucket != ''
        run: |
          BUCKET_RAW="${{ steps.get-bucket.outputs.bucket }}"
          BUCKET=$(echo "$BUCKET_RAW" | sed 's|^gs://||')
          BUCKET_PATH="gs://$BUCKET"
          
          echo "üì§ Subiendo DAGs actualizados a Composer..."
          
          if [ ! -d "airflow/dags" ]; then
            echo "‚ùå ERROR: Directorio airflow/dags no existe"
            exit 1
          fi
          
          for dag_file in airflow/dags/*.py; do
            if [ -f "$dag_file" ]; then
              filename=$(basename "$dag_file")
              echo "üì§ Subiendo $filename..."
              gsutil cp "$dag_file" "$BUCKET_PATH/dags/$filename"
              echo "   ‚úÖ $filename subido"
            fi
          done
          
          echo "‚úÖ DAGs actualizados"
        continue-on-error: false
      
      - name: Upload dbt to Composer (if changed)
        if: steps.get-bucket.outputs.bucket != '' && needs.detect-changes.outputs.dbt == 'true'
        run: |
          BUCKET_RAW="${{ steps.get-bucket.outputs.bucket }}"
          BUCKET=$(echo "$BUCKET_RAW" | sed 's|^gs://||')
          BUCKET_PATH="gs://$BUCKET"
          
          echo "üì§ Subiendo dbt actualizado a Composer..."
          
          TEMP_DBT_DIR=$(mktemp -d)
          cp -r dbt/* "$TEMP_DBT_DIR/" 2>/dev/null || true
          
          printf '%s\n' \
            'chicago_taxi_analysis:' \
            '  target: dev' \
            '  outputs:' \
            '    dev:' \
            '      type: bigquery' \
            '      method: oauth' \
            '      project: "{{ env_var('\''GCP_PROJECT_ID'\'') }}"' \
            '      dataset: "{{ env_var('\''DBT_DATASET'\'', '\''chicago_taxi_silver'\'') }}"' \
            '      location: us-central1' \
            '      threads: 4' \
            '      timeout_seconds: 300' \
            '      priority: interactive' \
            '      maximum_bytes_billed: 1000000000' \
            > "$TEMP_DBT_DIR/profiles.yml"
          
          gsutil -m cp -r "$TEMP_DBT_DIR"/* "$BUCKET_PATH/data/dbt/" || true
          rm -rf "$TEMP_DBT_DIR"
          
          echo "‚úÖ dbt actualizado"
        continue-on-error: true
      
      - name: Summary
        run: |
          echo "‚úÖ Actualizaci√≥n r√°pida completada"
          echo ""
          echo "üìã Archivos actualizados:"
          if [ "${{ needs.detect-changes.outputs.airflow_dags }}" == "true" ]; then
            echo "   - DAGs de Airflow"
          fi
          if [ "${{ needs.detect-changes.outputs.dbt }}" == "true" ]; then
            echo "   - Proyecto dbt"
          fi
          echo ""
          echo "‚è≥ Espera 1-2 minutos para que Airflow parsee los DAGs"
          echo "   Luego ejecuta el DAG en Airflow UI"

  setup-airflow:
    name: Setup Airflow (Cloud Composer)
    runs-on: ubuntu-latest
    needs: deploy-terraform
    if: needs.deploy-terraform.result == 'success' || needs.deploy-terraform.result == 'skipped'
    defaults:
      run:
        working-directory: ./
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 2  # Necesario para detectar cambios
      
      - name: Use detected changes
        id: changes
        run: |
          # Usar los cambios detectados en el job detect-changes
          echo "airflow=${{ needs.detect-changes.outputs.airflow_dags }}" >> $GITHUB_OUTPUT
          echo "dbt=${{ needs.detect-changes.outputs.dbt }}" >> $GITHUB_OUTPUT
          echo "airflow_config=${{ needs.detect-changes.outputs.airflow_config }}" >> $GITHUB_OUTPUT
      
      - name: Setup GCP Auth
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Setup GCP Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
      
      - name: Enable Composer API
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          echo "üîß Habilitando Cloud Composer API..."
          gcloud services enable composer.googleapis.com --project="$PROJECT_ID" || echo "‚ö†Ô∏è  API ya habilitada o error"
          echo "‚è≥ Esperando propagaci√≥n de API (10 segundos)..."
          sleep 10
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        continue-on-error: true
      
      - name: Grant Composer permissions to service account
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          SA_EMAIL=$(echo '${{ secrets.GCP_SA_KEY }}' | jq -r '.client_email')
          
          echo "üîê Otorgando permisos de Composer al service account..."
          gcloud projects add-iam-policy-binding "$PROJECT_ID" \
            --member="serviceAccount:$SA_EMAIL" \
            --role="roles/composer.admin" \
            --condition=None 2>&1 || echo "‚ö†Ô∏è  Error otorgando permisos (puede que ya existan)"
          
          echo "‚è≥ Esperando propagaci√≥n de permisos (5 segundos)..."
          sleep 5
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        continue-on-error: true
      
      - name: Grant permissions to Cloud Composer Service Agent
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          
          # Obtener el PROJECT_NUMBER del proyecto
          echo "üîç Obteniendo PROJECT_NUMBER del proyecto..."
          PROJECT_NUMBER=$(gcloud projects describe "$PROJECT_ID" --format="value(projectNumber)" 2>/dev/null || echo "")
          
          if [ -z "$PROJECT_NUMBER" ]; then
            echo "‚ùå No se pudo obtener PROJECT_NUMBER. El service agent no se puede configurar autom√°ticamente."
            echo "   Por favor, otorga manualmente el rol roles/composer.ServiceAgentV2Ext al service agent:"
            echo "   service-{PROJECT_NUMBER}@cloudcomposer-accounts.iam.gserviceaccount.com"
            echo "   Puedes encontrar el PROJECT_NUMBER en: https://console.cloud.google.com/iam-admin/settings?project=$PROJECT_ID"
            exit 1
          fi
          
          COMPOSER_SERVICE_AGENT="service-${PROJECT_NUMBER}@cloudcomposer-accounts.iam.gserviceaccount.com"
          echo "üîê Otorgando permisos al Cloud Composer Service Agent: $COMPOSER_SERVICE_AGENT"
          echo "   Rol: roles/composer.ServiceAgentV2Ext"
          
          gcloud projects add-iam-policy-binding "$PROJECT_ID" \
            --member="serviceAccount:$COMPOSER_SERVICE_AGENT" \
            --role="roles/composer.ServiceAgentV2Ext" \
            --condition=None 2>&1 || echo "‚ö†Ô∏è  Error otorgando permisos (puede que ya existan)"
          
          echo "‚è≥ Esperando propagaci√≥n de permisos (5 segundos)..."
          sleep 5
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        continue-on-error: false
      
      - name: Verify billing is enabled
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          
          echo "üí∞ Verificando que el proyecto tenga billing habilitado..."
          echo "   (BigQuery requiere billing incluso para datasets p√∫blicos)"
          
          BILLING_ACCOUNT=$(gcloud billing projects describe "$PROJECT_ID" \
            --format="value(billingAccountName)" 2>/dev/null || echo "")
          
          if [ -z "$BILLING_ACCOUNT" ] || [ "$BILLING_ACCOUNT" == "" ]; then
            echo ""
            echo "=========================================="
            echo "‚ö†Ô∏è  ADVERTENCIA: Billing no configurado"
            echo "=========================================="
            echo ""
            echo "El proyecto NO tiene billing habilitado."
            echo "BigQuery requiere billing incluso para datasets p√∫blicos."
            echo ""
            echo "üîß Soluci√≥n:"
            echo "   1. Ve a: https://console.cloud.google.com/billing?project=$PROJECT_ID"
            echo "   2. Link una cuenta de billing al proyecto"
            echo ""
            echo "=========================================="
            echo ""
          else
            echo "‚úÖ Billing habilitado: $BILLING_ACCOUNT"
          fi
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        continue-on-error: true
      
      - name: Grant BigQuery permissions to GitHub Actions service account
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          SA_EMAIL=$(echo '${{ secrets.GCP_SA_KEY }}' | jq -r '.client_email')
          
          echo "üîê Otorgando permisos de BigQuery al service account de GitHub Actions..."
          echo "   Service Account: $SA_EMAIL"
          echo "   (Este se usa para CREAR Composer y se especifica como --service-account)"
          
          # Roles necesarios para BigQuery (datasets p√∫blicos + crear tablas)
          # BigQuery User = incluye Data Viewer + Job User + Data Editor b√°sico
          for ROLE in "roles/bigquery.user" "roles/bigquery.dataViewer" "roles/bigquery.dataEditor" "roles/bigquery.jobUser"; do
            echo "   Otorgando rol: $ROLE"
            gcloud projects add-iam-policy-binding "$PROJECT_ID" \
              --member="serviceAccount:$SA_EMAIL" \
              --role="$ROLE" \
              --condition=None 2>&1 || echo "‚ö†Ô∏è  Error otorgando $ROLE (puede que ya exista)"
          done
          
          echo "‚úÖ Permisos otorgados al service account de GitHub Actions"
          echo "‚è≥ Esperando propagaci√≥n de permisos (10 segundos)..."
          sleep 10
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        continue-on-error: true
      
      - name: Grant BigQuery permissions to Composer service accounts
        if: steps.get-bucket.outputs.bucket != ''
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          SA_EMAIL=$(echo '${{ secrets.GCP_SA_KEY }}' | jq -r '.client_email')
          
          echo "üîê Otorgando permisos de BigQuery a TODOS los service accounts de Composer..."
          echo ""
          
          # Obtener project number para calcular service accounts por defecto
          PROJECT_NUMBER=$(gcloud projects describe "$PROJECT_ID" --format="value(projectNumber)" 2>/dev/null || echo "")
          
          # Lista de service accounts que pueden estar usando Composer
          SERVICE_ACCOUNTS=()
          
          # 1. Service account expl√≠cito de Composer (el que configuramos)
          COMPOSER_SA=$(gcloud composer environments describe "$COMPOSER_ENV" \
            --location "$REGION" \
            --project "$PROJECT_ID" \
            --format="value(config.nodeConfig.serviceAccount)" 2>/dev/null || echo "")
          
          if [ -n "$COMPOSER_SA" ] && [ "$COMPOSER_SA" != "" ]; then
            SERVICE_ACCOUNTS+=("$COMPOSER_SA")
            echo "   ‚úÖ Service Account expl√≠cito de Composer: $COMPOSER_SA"
          fi
          
          # 2. Compute Engine default service account (el que aparece como "default" en logs)
          if [ -n "$PROJECT_NUMBER" ]; then
            COMPUTE_SA="${PROJECT_NUMBER}-compute@developer.gserviceaccount.com"
            SERVICE_ACCOUNTS+=("$COMPUTE_SA")
            echo "   ‚úÖ Compute Engine Default SA: $COMPUTE_SA"
          fi
          
          # 3. Service account de GitHub Actions (por si acaso)
          SERVICE_ACCOUNTS+=("$SA_EMAIL")
          echo "   ‚úÖ GitHub Actions SA: $SA_EMAIL"
          
          echo ""
          echo "   IMPORTANTE: BigQuery requiere:"
          echo "   1. ‚úÖ Proyecto con billing habilitado"
          echo "   2. ‚úÖ Service account con permisos BigQuery"
          echo ""
          
          # Roles necesarios para BigQuery
          ROLES=(
            "roles/bigquery.user"
            "roles/bigquery.dataViewer"
            "roles/bigquery.dataEditor"
            "roles/bigquery.jobUser"
          )
          
          # Otorgar permisos a TODOS los service accounts
          for SA in "${SERVICE_ACCOUNTS[@]}"; do
            echo ""
            echo "üîê Otorgando permisos a: $SA"
            for ROLE in "${ROLES[@]}"; do
              echo "   - $ROLE"
              gcloud projects add-iam-policy-binding "$PROJECT_ID" \
                --member="serviceAccount:$SA" \
                --role="$ROLE" \
                --condition=None 2>&1 | grep -v "Updated IAM policy" || echo "     ‚úÖ Rol otorgado (o ya exist√≠a)"
            done
          done
          
          echo ""
          echo "‚úÖ Permisos otorgados a todos los service accounts"
          echo "‚è≥ Esperando propagaci√≥n de permisos (30 segundos)..."
          sleep 30
          
          echo ""
          echo "üìã Service accounts configurados:"
          for SA in "${SERVICE_ACCOUNTS[@]}"; do
            echo "   - $SA"
          done
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
        continue-on-error: true
      
      - name: Check if Composer environment exists
        id: check-composer
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          echo "üîç Verificando si existe el entorno de Composer..."
          
          # Verificar si existe (puede estar en cualquier estado: RUNNING, CREATING, UPDATING, etc.)
          ENV_INFO=$(gcloud composer environments describe "$COMPOSER_ENV" \
            --location "$REGION" \
            --project "$PROJECT_ID" \
            --format="value(name,state)" 2>/dev/null || echo "")
          
          if [ -n "$ENV_INFO" ]; then
            ENV_STATE=$(echo "$ENV_INFO" | awk '{print $NF}')
            echo "‚úÖ Composer environment existe (estado: $ENV_STATE)"
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "state=$ENV_STATE" >> $GITHUB_OUTPUT
            
            # Si est√° cre√°ndose, no intentar crearlo de nuevo
            if [ "$ENV_STATE" == "CREATING" ] || [ "$ENV_STATE" == "UPDATING" ]; then
              echo "‚ö†Ô∏è  Composer est√° en proceso de creaci√≥n/actualizaci√≥n. No se intentar√° crear de nuevo."
              echo "creating=true" >> $GITHUB_OUTPUT
            else
              echo "creating=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "‚ö†Ô∏è  Composer environment no existe"
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "creating=false" >> $GITHUB_OUTPUT
          fi
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
        continue-on-error: false
      
      - name: Create Composer environment (if needed)
        id: create-composer
        if: steps.check-composer.outputs.exists != 'true' && steps.check-composer.outputs.creating != 'true'
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          # Obtener el email del service account de GitHub Actions
          SA_EMAIL=$(echo '${{ secrets.GCP_SA_KEY }}' | jq -r '.client_email')
          
          echo "üîÑ Creando entorno de Cloud Composer..."
          echo "‚ö†Ô∏è  NOTA: Esto puede tardar 20-30 minutos."
          echo "   El workflow NO esperar√° a que termine - Composer se crear√° en background."
          echo "   Puedes verificar el progreso en:"
          echo "   https://console.cloud.google.com/composer/environments?project=$PROJECT_ID"
          echo ""
          echo "üîí IMPORTANTE: Si este workflow se ejecuta de nuevo mientras Composer se est√° creando,"
          echo "   NO intentar√° crearlo de nuevo (se detectar√° que est√° en estado CREATING)."
          echo ""
          echo "üìã Service Account para Composer: $SA_EMAIL"
          
          # Verificar una vez m√°s antes de crear (por si acaso cambi√≥ entre el check y ahora)
          ENV_CHECK=$(gcloud composer environments describe "$COMPOSER_ENV" \
            --location "$REGION" \
            --project "$PROJECT_ID" \
            --format="value(name)" 2>/dev/null || echo "")
          
          if [ -n "$ENV_CHECK" ]; then
            echo "‚úÖ Composer ya existe (fue creado entre el check y ahora). Saltando creaci√≥n."
            echo "created=false" >> $GITHUB_OUTPUT
            echo "status=exists" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Intentar crear (puede fallar si ya existe o por permisos)
          # NOTA: Composer 2.x requiere --service-account expl√≠cito
          # NOTA: Composer 2.x no acepta --machine-type, --disk-size, --python-version, ni --node-count
          # Usamos --async para que no espere (evita timeout del workflow)
          echo "üîÑ Ejecutando comando de creaci√≥n..."
          
          # Ejecutar el comando y capturar output y exit code
          # Usar temporal file para asegurar que capturamos todo el output
          TEMP_OUTPUT=$(mktemp)
          set +e  # Desactivar error estricto temporalmente
          gcloud composer environments create "$COMPOSER_ENV" \
            --location "$REGION" \
            --project "$PROJECT_ID" \
            --image-version composer-2.16.3-airflow-2.10.5 \
            --service-account "$SA_EMAIL" \
            --async > "$TEMP_OUTPUT" 2>&1
          CREATE_EXIT=$?
          CREATE_OUTPUT=$(cat "$TEMP_OUTPUT")
          rm -f "$TEMP_OUTPUT"
          set -e  # Reactivar error estricto
          
          # SIEMPRE mostrar el output para debugging
          echo ""
          echo "üìã Output del comando gcloud composer create:"
          echo "----------------------------------------"
          echo "$CREATE_OUTPUT"
          echo "----------------------------------------"
          echo "Exit code: $CREATE_EXIT"
          echo ""
          
          if [ $CREATE_EXIT -eq 0 ]; then
            echo "‚úÖ Comando de creaci√≥n de Composer iniciado (ejecut√°ndose en background)"
            echo "created=true" >> $GITHUB_OUTPUT
            echo "status=creating" >> $GITHUB_OUTPUT
          else
            # Analizar el error
            if echo "$CREATE_OUTPUT" | grep -qi "already exists\|Already exists"; then
              echo "‚úÖ Composer ya existe. Continuando..."
              echo "created=false" >> $GITHUB_OUTPUT
              echo "status=exists" >> $GITHUB_OUTPUT
            else
              echo ""
              echo "=========================================="
              echo "‚ùå ERROR CR√çTICO: No se pudo crear Composer"
              echo "=========================================="
              echo ""
              echo "Detalles del error:"
              echo "$CREATE_OUTPUT"
              echo ""
              echo "Posibles causas:"
              echo "  1. Falta el rol roles/composer.admin en el service account"
              echo "  2. La API composer.googleapis.com no est√° habilitada"
              echo "  3. Cuota insuficiente en el proyecto"
              echo "  4. Error de permisos o configuraci√≥n"
              echo ""
              echo "üîç Verifica en la consola:"
              echo "   https://console.cloud.google.com/composer/environments?project=$PROJECT_ID"
              echo ""
              echo "üîß Soluci√≥n:"
              echo "   1. Verifica los permisos del service account"
              echo "   2. Habilita la API: gcloud services enable composer.googleapis.com"
              echo "   3. Revisa las cuotas del proyecto"
              echo ""
              echo "=========================================="
              echo "created=false" >> $GITHUB_OUTPUT
              echo "status=error" >> $GITHUB_OUTPUT
              exit 1  # FALLAR el workflow para que se notifique
            fi
          fi
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
        continue-on-error: false
      
      - name: Get Composer bucket (with retries)
        id: get-bucket
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          # Si hubo error al crear, verificar el estado antes de intentar obtener el bucket
          if [ "${{ steps.create-composer.outputs.status }}" == "error" ]; then
            echo ""
            echo "=========================================="
            echo "‚ùå ERROR: Composer no se pudo crear"
            echo "=========================================="
            echo ""
            echo "El workflow fallar√° para notificar el error."
            echo "Verifica los logs del step anterior para m√°s detalles."
            echo ""
            exit 1
          fi
          
          echo "üîç Obteniendo bucket de Composer (con reintentos)..."
          
          # Si se acaba de crear, esperar un poco antes de intentar obtener el bucket
          if [ "${{ steps.create-composer.outputs.status }}" == "creating" ]; then
            echo "‚è≥ Composer se est√° creando. Esperando 60 segundos antes de intentar obtener el bucket..."
            sleep 60
          fi
          
          # Intentar hasta 6 veces (3 minutos total)
          for i in {1..6}; do
            echo "   Intento $i/6..."
            
            # Verificar el estado de Composer
            ENV_STATE=$(gcloud composer environments describe "$COMPOSER_ENV" \
              --location "$REGION" \
              --project "$PROJECT_ID" \
              --format="value(state)" 2>/dev/null || echo "NOT_FOUND")
            
            if [ "$ENV_STATE" == "NOT_FOUND" ]; then
              echo "   ‚ö†Ô∏è  Composer no encontrado. Puede estar a√∫n cre√°ndose o hubo un error."
            elif [ "$ENV_STATE" == "CREATING" ] || [ "$ENV_STATE" == "UPDATING" ]; then
              echo "   ‚è≥ Composer est√° en estado: $ENV_STATE (a√∫n no est√° listo)"
            elif [ "$ENV_STATE" == "RUNNING" ]; then
              echo "   ‚úÖ Composer est√° RUNNING. Obteniendo bucket..."
            elif [ "$ENV_STATE" == "ERROR" ] || [ "$ENV_STATE" == "FAILED" ]; then
              echo ""
              echo "=========================================="
              echo "‚ùå ERROR: Composer fall√≥ al crearse"
              echo "=========================================="
              echo ""
              echo "Estado: $ENV_STATE"
              echo ""
              echo "üîç Verifica en la consola:"
              echo "   https://console.cloud.google.com/composer/environments?project=$PROJECT_ID"
              echo ""
              echo "El workflow fallar√° para notificar el error."
              echo ""
              exit 1
            else
              echo "   ‚ÑπÔ∏è  Estado de Composer: $ENV_STATE"
            fi
            
            BUCKET=$(gcloud composer environments describe "$COMPOSER_ENV" \
              --location "$REGION" \
              --project "$PROJECT_ID" \
              --format="value(config.dagGcsPrefix)" 2>/dev/null | sed 's|/dags||' || echo "")
            
            if [ -n "$BUCKET" ]; then
              echo "‚úÖ Bucket encontrado: $BUCKET"
              echo "bucket=$BUCKET" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            if [ $i -lt 6 ]; then
              echo "   Esperando 30 segundos antes del siguiente intento..."
              sleep 30
            fi
          done
          
          echo ""
          echo "=========================================="
          echo "‚ö†Ô∏è  ADVERTENCIA: No se pudo obtener el bucket de Composer"
          echo "=========================================="
          echo ""
          echo "Esto puede significar:"
          echo "  1. Composer a√∫n se est√° creando (tarda 20-30 minutos)"
          echo "  2. Hubo un error en la creaci√≥n (verifica en la consola)"
          echo ""
          echo "üìã Pr√≥ximos pasos:"
          echo "1. Verifica el estado en:"
          echo "   https://console.cloud.google.com/composer/environments?project=$PROJECT_ID"
          echo ""
          echo "2. Si Composer est√° CREATING, espera 20-30 minutos y vuelve a ejecutar el workflow"
          echo ""
          echo "3. Si Composer est√° en ERROR, revisa los logs en la consola"
          echo ""
          echo "‚ö†Ô∏è  Continuando sin subir DAGs (se pueden subir despu√©s manualmente)"
          echo "bucket=" >> $GITHUB_OUTPUT
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
        continue-on-error: true
      
      - name: Determine if uploads are needed
        id: should-upload
        run: |
          # Subir archivos SIEMPRE cuando:
          # 1. Composer est√° listo (bucket disponible), Y
          # 2. Es ejecuci√≥n manual, O hay cambios, O Composer existe (para asegurar que los DAGs est√©n)
          COMPOSER_EXISTS="${{ steps.check-composer.outputs.exists == 'true' }}"
          AIRFLOW_CHANGED="${{ steps.changes.outputs.airflow == 'true' }}"
          DBT_CHANGED="${{ steps.changes.outputs.dbt == 'true' }}"
          CONFIG_CHANGED="${{ steps.changes.outputs.airflow_config == 'true' }}"
          IS_MANUAL="${{ github.event_name == 'workflow_dispatch' }}"
          
          echo "üîç Evaluando si se deben subir archivos:"
          echo "   - Composer existe: $COMPOSER_EXISTS"
          echo "   - Cambios en Airflow: $AIRFLOW_CHANGED"
          echo "   - Cambios en dbt: $DBT_CHANGED"
          echo "   - Cambios en config: $CONFIG_CHANGED"
          echo "   - Ejecuci√≥n manual: $IS_MANUAL"
          echo ""
          
          # SIEMPRE subir si Composer existe (para asegurar que los DAGs est√©n actualizados)
          if [ "$COMPOSER_EXISTS" == "true" ]; then
            echo "‚úÖ Composer existe - subiendo archivos para asegurar que los DAGs est√©n presentes y actualizados"
            echo "upload=true" >> $GITHUB_OUTPUT
          # Si es ejecuci√≥n manual, SIEMPRE subir
          elif [ "$IS_MANUAL" == "true" ]; then
            echo "‚úÖ Ejecuci√≥n manual - se actualizar√°n los archivos"
            echo "upload=true" >> $GITHUB_OUTPUT
          # Si hay cambios, subir
          elif [ "$AIRFLOW_CHANGED" == "true" ] || \
               [ "$DBT_CHANGED" == "true" ] || \
               [ "$CONFIG_CHANGED" == "true" ]; then
            echo "‚úÖ Cambios detectados - se actualizar√°n los archivos"
            echo "upload=true" >> $GITHUB_OUTPUT
          else
            echo "‚è≠Ô∏è  No se cumplen condiciones para subir archivos"
            echo "upload=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Upload DAGs to Composer
        if: steps.get-bucket.outputs.bucket != '' && steps.should-upload.outputs.upload == 'true'
        run: |
          BUCKET_RAW="${{ steps.get-bucket.outputs.bucket }}"
          
          # Normalizar el bucket: remover gs:// si est√° presente y luego agregarlo
          BUCKET=$(echo "$BUCKET_RAW" | sed 's|^gs://||')
          BUCKET_PATH="gs://$BUCKET"
          
          echo "üì§ Subiendo DAGs a Composer..."
          echo "   Bucket raw: $BUCKET_RAW"
          echo "   Bucket normalizado: $BUCKET"
          echo "   Destino: $BUCKET_PATH/dags/"
          echo ""
          
          # Verificar que los archivos existen
          if [ ! -d "airflow/dags" ]; then
            echo "‚ùå ERROR: Directorio airflow/dags no existe"
            exit 1
          fi
          
          # Listar archivos que se van a subir
          echo "üìã Archivos DAG encontrados:"
          ls -la airflow/dags/*.py || echo "‚ö†Ô∏è  No se encontraron archivos .py"
          echo ""
          
          # Subir cada archivo .py individualmente para mejor control
          for dag_file in airflow/dags/*.py; do
            if [ -f "$dag_file" ]; then
              filename=$(basename "$dag_file")
              echo "üì§ Subiendo $filename..."
              gsutil cp "$dag_file" "$BUCKET_PATH/dags/$filename"
              if [ $? -eq 0 ]; then
                echo "   ‚úÖ $filename subido correctamente"
              else
                echo "   ‚ùå Error subiendo $filename"
                exit 1
              fi
            fi
          done
          
          # Subir requirements.txt si existe
          if [ -f "airflow/dags/requirements.txt" ]; then
            echo "üì§ Subiendo requirements.txt..."
            gsutil cp airflow/dags/requirements.txt "$BUCKET_PATH/data/requirements.txt"
            echo "   ‚úÖ requirements.txt subido"
          fi
          
          echo ""
          echo "‚úÖ Todos los DAGs subidos correctamente"
          
          # Verificar que se subieron
          echo ""
          echo "üîç Verificando DAGs en el bucket:"
          gsutil ls "$BUCKET_PATH/dags/*.py" || echo "‚ö†Ô∏è  No se encontraron DAGs en el bucket"
        continue-on-error: false
      
      - name: Upload dbt to Composer
        if: steps.get-bucket.outputs.bucket != '' && steps.should-upload.outputs.upload == 'true'
        run: |
          BUCKET_RAW="${{ steps.get-bucket.outputs.bucket }}"
          # Normalizar el bucket: remover gs:// si est√° presente y luego agregarlo
          BUCKET=$(echo "$BUCKET_RAW" | sed 's|^gs://||')
          BUCKET_PATH="gs://$BUCKET"
          echo "üì§ Subiendo dbt a Composer..."
          
          # Crear directorio temporal para dbt con profiles.yml correcto
          TEMP_DBT_DIR=$(mktemp -d)
          cp -r dbt/* "$TEMP_DBT_DIR/" 2>/dev/null || true
          
          # Crear profiles.yml para Airflow (usa oauth para acceder a datasets p√∫blicos)
          # Usar Application Default Credentials (ADC) de Airflow para acceder a datasets p√∫blicos
          # Usar printf para evitar problemas con {{ }} en YAML
          printf '%s\n' \
            'chicago_taxi_analysis:' \
            '  target: dev' \
            '  outputs:' \
            '    dev:' \
            '      type: bigquery' \
            '      method: oauth' \
            '      project: "{{ env_var('\''GCP_PROJECT_ID'\'') }}"' \
            '      dataset: "{{ env_var('\''DBT_DATASET'\'', '\''chicago_taxi_silver'\'') }}"' \
            '      location: us-central1' \
            '      threads: 4' \
            '      timeout_seconds: 300' \
            '      priority: interactive' \
            '      maximum_bytes_billed: 1000000000' \
            > "$TEMP_DBT_DIR/profiles.yml"
          
          echo "üìã profiles.yml creado para Airflow (usa oauth/ADC para acceder a datasets p√∫blicos)"
          echo "   M√©todo: oauth (usa Application Default Credentials de Airflow)"
          
          # Subir dbt con profiles.yml correcto
          gsutil -m cp -r "$TEMP_DBT_DIR"/* "$BUCKET_PATH/data/dbt/" || true
          rm -rf "$TEMP_DBT_DIR"
          echo "‚úÖ dbt subido con profiles.yml configurado para oauth (ADC)"
        continue-on-error: true
      
      - name: Upload service account key to Composer
        if: steps.get-bucket.outputs.bucket != '' && steps.should-upload.outputs.upload == 'true'
        run: |
          BUCKET_RAW="${{ steps.get-bucket.outputs.bucket }}"
          # Normalizar el bucket: remover gs:// si est√° presente y luego agregarlo
          BUCKET=$(echo "$BUCKET_RAW" | sed 's|^gs://||')
          BUCKET_PATH="gs://$BUCKET"
          
          echo "üì§ Creando service account key desde secret..."
          # Crear el archivo desde el secret
          echo '${{ secrets.GCP_SA_KEY }}' > github-actions-key.json
          
          if [ -f "github-actions-key.json" ]; then
            echo "‚úÖ Service account key creado"
            echo "üì§ Subiendo service account key a Composer..."
            gsutil cp github-actions-key.json "$BUCKET_PATH/data/github-actions-key.json" || {
              echo "‚ö†Ô∏è  Error subiendo service account key, pero continuando..."
              exit 0
            }
            echo "‚úÖ Service account key subido a $BUCKET_PATH/data/github-actions-key.json"
            
            # Verificar que se subi√≥ correctamente
            gsutil ls "$BUCKET_PATH/data/github-actions-key.json" && echo "‚úÖ Verificado: archivo existe en GCS"
            
            # Limpiar archivo local (no dejar credenciales en el runner)
            rm -f github-actions-key.json
          else
            echo "‚ùå Error: No se pudo crear github-actions-key.json"
            exit 1
          fi
        continue-on-error: true
      
      - name: Skip uploads (no changes or Composer not ready)
        if: steps.get-bucket.outputs.bucket == '' || steps.should-upload.outputs.upload != 'true'
        run: |
          if [ "${{ steps.get-bucket.outputs.bucket }}" == "" ]; then
            echo "‚è≠Ô∏è  Saltando subida de archivos - Composer a√∫n no est√° listo"
            echo "   El entorno puede estar a√∫n cre√°ndose (tarda 20-30 minutos)"
          else
            echo "‚è≠Ô∏è  Saltando subida de archivos - No hay cambios en archivos de Airflow"
            echo "   Los archivos solo se actualizan cuando hay cambios en:"
            echo "   - airflow/**"
            echo "   - dbt/**"
            echo "   - .github/workflows/cd.yml"
          fi
      
      - name: Set Airflow variables
        if: steps.get-bucket.outputs.bucket != '' && (steps.should-upload.outputs.upload == 'true' || steps.check-composer.outputs.exists != 'true')
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          echo "üîß Configurando variables de Airflow..."
          echo "   NOTA: Cloud Composer no soporta 'variables --set' directamente"
          echo "   Usando 'airflow variables set' uno por uno (puede tardar ~1 minuto por variable)"
          echo ""
          
          # Funci√≥n helper para configurar variable con manejo de errores
          set_var() {
            local var_name=$1
            local var_value=$2
            echo "   Configurando $var_name..."
            gcloud composer environments run "$COMPOSER_ENV" \
              --location "$REGION" \
              variables -- \
              set "$var_name" "$var_value" 2>&1 | grep -v "Use ctrl-c to interrupt" || {
              echo "   ‚ö†Ô∏è  Error o variable ya existe: $var_name"
              return 0  # No fallar si la variable ya existe
            }
          }
          
          # Configurar variables una por una
          # NOTA: Cada comando puede tardar ~1 minuto, pero es el m√©todo que funciona
          set_var "GCP_PROJECT_ID" "$PROJECT_ID"
          set_var "GCP_REGION" "$REGION"
          set_var "DBT_PROJECT_DIR" "/home/airflow/gcs/data/dbt"
          set_var "DBT_PROFILES_DIR" "/home/airflow/gcs/data/dbt"
          set_var "DBT_PROFILE" "chicago_taxi_analysis"
          set_var "GCP_SA_KEY_PATH" "/home/airflow/gcs/data/github-actions-key.json"
          
          if [ -n "${{ secrets.OPENWEATHER_API_KEY || '' }}" ]; then
            set_var "OPENWEATHER_API_KEY" "${{ secrets.OPENWEATHER_API_KEY }}"
          else
            echo "   ‚è≠Ô∏è  Saltando OPENWEATHER_API_KEY (no configurado en secrets)"
          fi
          
          echo ""
          echo "‚úÖ Variables configuradas (puede que algunas ya existieran)"
          echo "   Verifica en Airflow UI: Admin ‚Üí Variables"
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
        continue-on-error: true
      
      - name: Wait for DAGs to be parsed by Airflow
        if: steps.get-bucket.outputs.bucket != '' && steps.should-upload.outputs.upload == 'true'
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          echo "‚è≥ Esperando 60 segundos para que Airflow parsee los DAGs..."
          sleep 60
          
          echo "‚úÖ Tiempo de espera completado"
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
        continue-on-error: true
      
      - name: Unpause daily DAG and trigger historical DAG
        if: steps.get-bucket.outputs.bucket != '' && steps.should-upload.outputs.upload == 'true'
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          echo "üöÄ Activando y ejecutando DAGs..."
          
          # Unpause el DAG diario para que se ejecute autom√°ticamente
          echo "üìÖ Activando DAG diario (chicago_taxi_daily_pipeline)..."
          gcloud composer environments run "$COMPOSER_ENV" \
            --location "$REGION" \
            dags unpause chicago_taxi_daily_pipeline 2>&1 || echo "‚ö†Ô∏è  Error activando DAG diario (puede que ya est√© activo)"
          
          # Trigger el DAG hist√≥rico (solo una vez)
          echo "üìä Ejecutando DAG hist√≥rico (chicago_taxi_historical_ingestion)..."
          gcloud composer environments run "$COMPOSER_ENV" \
            --location "$REGION" \
            dags trigger chicago_taxi_historical_ingestion 2>&1 || echo "‚ö†Ô∏è  Error ejecutando DAG hist√≥rico (puede que ya est√© corriendo)"
          
          echo "‚úÖ DAGs configurados y ejecutados"
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
        continue-on-error: true
      
      - name: Airflow setup summary
        run: |
          PROJECT_ID="${{ secrets.GCP_PROJECT_ID }}"
          REGION="${{ secrets.GCP_REGION || 'us-central1' }}"
          COMPOSER_ENV="chicago-taxi-composer"
          
          echo ""
          echo "=========================================="
          echo "‚úÖ AIRFLOW CONFIGURADO Y EJECUTADO"
          echo "=========================================="
          echo ""
          echo "üìã Estado de los DAGs:"
          echo "‚úÖ DAG hist√≥rico (chicago_taxi_historical_ingestion): Ejecut√°ndose autom√°ticamente"
          echo "‚úÖ DAG diario (chicago_taxi_daily_pipeline): Activado - se ejecutar√° diariamente a las 2 AM UTC"
          echo ""
          echo "üîç Monitoreo:"
          echo "   Ve a Airflow UI:"
          echo "   https://console.cloud.google.com/composer/environments/$COMPOSER_ENV/monitoring?project=$PROJECT_ID"
          echo ""
          echo "üìä El DAG hist√≥rico se est√° ejecutando ahora y cargar√° todos los datos hist√≥ricos."
          echo "üìÖ El DAG diario se ejecutar√° autom√°ticamente todos los d√≠as."
          echo ""
          echo "=========================================="
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
          GCP_REGION: ${{ secrets.GCP_REGION || 'us-central1' }}
