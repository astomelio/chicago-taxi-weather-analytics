# Orbidi Data Engineer Challenge - Chicago Taxi Trips Analysis

> **Nota sobre el Desaf√≠o**: Este desaf√≠o consta de dos partes:
> - **Part 1: Design Challenge** - Requiere un diagrama de arquitectura en PDF (entregable separado)
> - **Part 2: Coding Challenge** - Este repositorio implementa la soluci√≥n de c√≥digo
> 
> Ver `Orbidi Data Engineer Technical Challenge.pdf` para los detalles completos del desaf√≠o.

## Descripci√≥n del Proyecto

Este proyecto analiza la relaci√≥n entre las condiciones clim√°ticas y la duraci√≥n de los viajes en taxis de Chicago. El alcalde de Chicago sospecha que el clima afecta la duraci√≥n de los viajes, por lo que se ha desarrollado un dashboard en Looker Studio para explorar esta hip√≥tesis.

**Este repositorio implementa el Part 2: Coding Challenge del desaf√≠o t√©cnico de Orbidi.**

## Arquitectura

### Componentes Principales

1. **Ingesta de Datos**
   - **Datos de Taxis**: Extra√≠dos directamente de BigQuery (dataset p√∫blico de Chicago)
   - **Datos del Clima**: Obtenidos desde dataset p√∫blico de NOAA en BigQuery (bigquery-public-data.noaa_gsod)
   - Pipeline programado con Cloud Scheduler y Cloud Functions

2. **Almacenamiento**
   - **BigQuery**: Data warehouse para almacenar datos raw y transformados
   - Capas de datos: Raw (Bronze) ‚Üí Silver ‚Üí Gold

3. **Transformaci√≥n**
   - **dbt**: Herramienta para transformaciones y modelado de datos
   - Eliminaci√≥n de duplicados en capa Silver
   - Agregaciones y joins en capa Gold

4. **Visualizaci√≥n**
   - **Looker Studio**: Dashboard interactivo para an√°lisis

5. **Seguridad**
   - Column-level security para `payment_type` (solo accesible por el email del desarrollador)

## Estructura del Proyecto

```
.
‚îú‚îÄ‚îÄ terraform/              # Infraestructura como c√≥digo
‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îî‚îÄ‚îÄ modules/
‚îú‚îÄ‚îÄ dbt/                    # Transformaciones de datos
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ profiles.yml
‚îÇ   ‚îî‚îÄ‚îÄ dbt_project.yml
‚îú‚îÄ‚îÄ airflow/                # Orquestaci√≥n con Airflow (RECOMENDADO)
‚îÇ   ‚îî‚îÄ‚îÄ dags/
‚îÇ       ‚îú‚îÄ‚îÄ chicago_taxi_pipeline.py
‚îÇ       ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ functions/              # Cloud Functions
‚îÇ   ‚îî‚îÄ‚îÄ weather_ingestion/
‚îú‚îÄ‚îÄ scripts/               # Scripts auxiliares
‚îÇ   ‚îú‚îÄ‚îÄ setup_airflow.sh
‚îÇ   ‚îî‚îÄ‚îÄ verify_tables.py
‚îú‚îÄ‚îÄ .github/               # CI/CD (solo para infraestructura)
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îî‚îÄ‚îÄ README.md
```

## üöÄ Inicio R√°pido - Todo Autom√°tico

**GitHub Actions hace TODO autom√°ticamente:**

1. **Configurar Secrets en GitHub**:
   - Ve a: `Settings > Secrets and variables > Actions`
   - Agrega: `GCP_SA_KEY`, `GCP_PROJECT_ID`, `DEVELOPER_EMAIL`
   - (Opcional): `OPENWEATHER_API_KEY`, `GCP_REGION`

2. **Hacer push a main**:
   ```bash
   git push origin main
   ```

3. **GitHub Actions autom√°ticamente**:
   - ‚úÖ Despliega infraestructura con Terraform (BigQuery, Cloud Functions, etc.)
   - ‚úÖ Crea entorno de Cloud Composer (si no existe)
   - ‚úÖ Sube DAGs de Airflow
   - ‚úÖ Sube c√≥digo dbt
   - ‚úÖ Configura variables de Airflow
   - ‚úÖ **TODO queda listo para usar**

4. **√öltimo paso manual (una vez)**:
   - Ve a Airflow UI (el link aparece en los logs de GitHub Actions)
   - Trigger el DAG `chicago_taxi_historical_ingestion`
   - El pipeline diario se ejecutar√° autom√°ticamente despu√©s

üìñ **Gu√≠a completa de Airflow**: Ver [airflow/README.md](airflow/README.md)

### ¬øQui√©n hace qu√©?

- **Terraform (via GitHub Actions)**: Crea infraestructura (BigQuery, Cloud Functions, Cloud Scheduler)
- **GitHub Actions**: Configura Airflow autom√°ticamente (crea Composer, sube DAGs, configura variables)
- **Airflow**: Ejecuta el pipeline de datos (ingesta, transformaciones dbt)
- **T√∫**: Solo necesitas trigger el DAG hist√≥rico una vez

---

## Requisitos Previos

- Cuenta de Google Cloud Platform con proyecto activo y facturaci√≥n habilitada
- Cuenta de GitHub (para despliegue autom√°tico)
- (Opcional) API key de clima solo si BigQuery p√∫blico no tiene datos para alguna fecha

## Configuraci√≥n Inicial

### Opci√≥n 1: Pipeline con Airflow (Recomendado para Datos)

Ver [airflow/README.md](airflow/README.md) para instrucciones completas.

### Opci√≥n 2: Despliegue de Infraestructura con GitHub Actions

**Pasos:**

1. **Crear Service Account en GCP** (ver [SETUP.md](SETUP.md#paso-2-crear-service-account-en-gcp))

2. **Configurar Secrets en GitHub**:
   - Ve a: `Settings > Secrets and variables > Actions`
   - Agrega: `GCP_SA_KEY`, `GCP_PROJECT_ID`, `DEVELOPER_EMAIL`
   - (Opcional): `OPENWEATHER_API_KEY`, `GCP_REGION`

3. **Hacer push a main**:
   ```bash
   git push origin main
   ```

**GitHub Actions autom√°ticamente:**
- ‚úÖ Habilita APIs necesarias
- ‚úÖ Crea ZIP de la funci√≥n
- ‚úÖ Ejecuta `terraform apply`
- ‚úÖ Despliega toda la infraestructura
- ‚úÖ Ejecuta modelos dbt

### Opci√≥n 2: Despliegue Manual (Alternativa)

```bash
cd terraform
terraform init
terraform plan
terraform apply
```

### 3. Configurar dbt

```bash
cd dbt
dbt deps
dbt debug
```

### 4. Ejecutar Ingesta de Datos del Clima

**Modo Hist√≥rico** (primera ejecuci√≥n - ingesta todos los datos de junio-diciembre 2023):
```bash
python functions/weather_ingestion/main.py --historical
```

**Modo Diario** (ejecuci√≥n diaria - ingesta solo el d√≠a anterior):
```bash
python functions/weather_ingestion/main.py
```

### 5. Ejecutar Transformaciones dbt

```bash
cd dbt
dbt run --models silver
dbt run --models gold
dbt test
```

### 6. Crear Dashboard en Looker Studio

Sigue las instrucciones en [DASHBOARD_SETUP.md](DASHBOARD_SETUP.md) para crear el dashboard.

**Nota**: Una vez creado, compartir el dashboard con:
- alejandro@astrafy.io
- felipe.bereilh@orbidi.com

Y agregar el link en este README.

## Filtros de Datos

- **Per√≠odo de an√°lisis**: 01/06/2023 - 31/12/2023 (6 meses)
- **Fuente de taxis**: BigQuery Public Dataset `bigquery-public-data.chicago_taxi_trips.taxi_trips`
- **Datos del clima**: 
  - Fuente: Dataset p√∫blico de NOAA en BigQuery (bigquery-public-data.noaa_gsod)
  - Hist√≥ricos: 01/06/2023 - 31/12/2023 (6 meses, para mantener queries en tier gratuito)
  - Diarios: Se ingieren autom√°ticamente cada d√≠a (aunque no se usen en el dashboard)

## Pipeline de Datos

### Flujo de Datos

1. **Raw Layer (Bronze)**
   - Datos de taxis extra√≠dos de BigQuery p√∫blico
   - Datos del clima obtenidos desde dataset p√∫blico de NOAA en BigQuery

2. **Silver Layer**
   - Limpieza y deduplicaci√≥n de datos
   - Validaci√≥n de esquemas
   - Enriquecimiento con metadatos

3. **Gold Layer**
   - Agregaciones por d√≠a/hora
   - Joins entre taxis y clima
   - M√©tricas calculadas (duraci√≥n promedio, etc.)

### Automatizaci√≥n

- **GitHub Actions**: Despliega autom√°ticamente cuando haces push a `main`
  - CI: Valida c√≥digo en PRs
  - CD: Despliega infraestructura y ejecuta dbt en `main`
- **Cloud Scheduler**: Ejecuta la funci√≥n de ingesta diaria a las 02:00 AM UTC
- **Cloud Functions**: Procesa la ingesta de datos del clima
- **dbt**: Transformaciones ejecutadas manualmente o v√≠a CI/CD

**Nota sobre escalabilidad**: Para esta prueba, Cloud Scheduler es suficiente. Si en el futuro se requieren m√∫ltiples ingestas diarias, procesos en batch complejos, o dependencias entre tareas, se recomienda migrar a Cloud Composer (Apache Airflow) para una orquestaci√≥n m√°s robusta.

## Seguridad

- Column-level security implementada para `payment_type`
- Solo el email del desarrollador tiene acceso a esta columna
- Implementado mediante pol√≠ticas de seguridad de BigQuery

## CI/CD

El proyecto incluye pipelines de CI/CD con GitHub Actions para:
- Validaci√≥n de c√≥digo Terraform
- Tests de modelos dbt
- Linting y formateo de c√≥digo

## Costos

Al usar solo 6 meses de datos (junio-diciembre 2023), las consultas permanecen dentro del tier gratuito de Google Cloud para proyectos nuevos.

## Estructura del Desaf√≠o

Este proyecto corresponde al **Part 2: Coding Challenge** del desaf√≠o t√©cnico de Orbidi.

### Part 1: Design Challenge
- **Entregable**: Diagrama de arquitectura en PDF
- **Tema**: Dise√±o de soluci√≥n anal√≠tica para cliente con m√∫ltiples fuentes de datos
  - Fuentes: PostgreSQL, MySQL, MongoDB, SAP, Salesforce, SurveyMonkey
  - Objetivos: BI dashboards y modelos ML
  - Requisitos: Data mesh, GitOps, DataOps, Google Cloud, tecnolog√≠as open-source
  - Dominios de datos: customers, products, maisons (extensible)
  - Gobernanza federada: acceso, observabilidad, cat√°logo
- **Estado**: üìã **DISE√ëO COMPLETADO** - Ver documentaci√≥n en `Part1_Architecture_Design.md`
- **Gu√≠a para diagrama**: Ver `Part1_Diagram_Guide.txt` para crear el diagrama en draw.io
- **Ubicaci√≥n esperada**: `Part1_Architecture_Diagram.pdf` (crear desde la gu√≠a)

### Part 2: Coding Challenge (Este Repositorio)
- **Entregable**: C√≥digo en GitHub + Dashboard en Looker Studio
- **Tema**: An√°lisis de relaci√≥n entre clima y duraci√≥n de viajes en taxis de Chicago
- **Requisitos**: Terraform, dbt, automatizaci√≥n, BigQuery, Looker Studio
- **Estado**: ‚úÖ Implementado y funcionando

## Autor

Desarrollado como parte del desaf√≠o t√©cnico de Data Engineer de Orbidi.
