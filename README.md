# Orbidi Data Engineer Challenge - Chicago Taxi Trips Analysis

## Descripci√≥n del Proyecto

Este proyecto analiza la relaci√≥n entre las condiciones clim√°ticas y la duraci√≥n de los viajes en taxis de Chicago. El alcalde de Chicago sospecha que el clima afecta la duraci√≥n de los viajes, por lo que se ha desarrollado un dashboard en Looker Studio para explorar esta hip√≥tesis.

## Arquitectura

### Componentes Principales

1. **Ingesta de Datos**
   - **Datos de Taxis**: Extra√≠dos directamente de BigQuery (dataset p√∫blico de Chicago)
   - **Datos del Clima**: Obtenidos desde dataset p√∫blico de NOAA en BigQuery (bigquery-public-data.noaa_gsod)
   - Pipeline programado con Cloud Scheduler y Cloud Functions

2. **Almacenamiento**
   - **BigQuery**: Data warehouse para almacenar datos raw y transformados
   - Capas de datos: Raw (Bronze) ‚Üí Silver ‚Üí Gold

3. **Transformaci√≥n**
   - **dbt**: Herramienta para transformaciones y modelado de datos
   - Eliminaci√≥n de duplicados en capa Silver
   - Agregaciones y joins en capa Gold

4. **Visualizaci√≥n**
   - **Looker Studio**: Dashboard interactivo para an√°lisis

5. **Seguridad**
   - Column-level security para `payment_type` (solo accesible por el email del desarrollador)

## Estructura del Proyecto

```
.
‚îú‚îÄ‚îÄ terraform/              # Infraestructura como c√≥digo
‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îî‚îÄ‚îÄ modules/
‚îú‚îÄ‚îÄ dbt/                    # Transformaciones de datos
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ profiles.yml
‚îÇ   ‚îî‚îÄ‚îÄ dbt_project.yml
‚îú‚îÄ‚îÄ functions/              # Cloud Functions
‚îÇ   ‚îî‚îÄ‚îÄ weather_ingestion/
‚îú‚îÄ‚îÄ scripts/               # Scripts auxiliares
‚îÇ   ‚îî‚îÄ‚îÄ setup.sh
‚îú‚îÄ‚îÄ .github/               # CI/CD
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îî‚îÄ‚îÄ README.md
```

## üöÄ Inicio R√°pido - Despliegue Autom√°tico

**Para desplegar TODO autom√°ticamente en tu cuenta de Google Cloud:**

1. **Clonar el repositorio**
2. **Configurar Secrets en GitHub** (ver [SETUP.md](SETUP.md))
3. **Hacer push a main**
4. **GitHub Actions despliega TODO autom√°ticamente**

üìñ **Gu√≠a completa**: Ver [SETUP.md](SETUP.md)

---

## Requisitos Previos

- Cuenta de Google Cloud Platform con proyecto activo y facturaci√≥n habilitada
- Cuenta de GitHub (para despliegue autom√°tico)
- (Opcional) API key de clima solo si BigQuery p√∫blico no tiene datos para alguna fecha

## Configuraci√≥n Inicial

### Opci√≥n 1: Despliegue Autom√°tico con GitHub Actions (Recomendado)

**Pasos:**

1. **Crear Service Account en GCP** (ver [SETUP.md](SETUP.md#paso-2-crear-service-account-en-gcp))

2. **Configurar Secrets en GitHub**:
   - Ve a: `Settings > Secrets and variables > Actions`
   - Agrega: `GCP_SA_KEY`, `GCP_PROJECT_ID`, `DEVELOPER_EMAIL`
   - (Opcional): `OPENWEATHER_API_KEY`, `GCP_REGION`

3. **Hacer push a main**:
   ```bash
   git push origin main
   ```

**GitHub Actions autom√°ticamente:**
- ‚úÖ Habilita APIs necesarias
- ‚úÖ Crea ZIP de la funci√≥n
- ‚úÖ Ejecuta `terraform apply`
- ‚úÖ Despliega toda la infraestructura
- ‚úÖ Ejecuta modelos dbt

### Opci√≥n 2: Despliegue Manual (Alternativa)

```bash
cd terraform
terraform init
terraform plan
terraform apply
```

### 3. Configurar dbt

```bash
cd dbt
dbt deps
dbt debug
```

### 4. Ejecutar Ingesta de Datos del Clima

**Modo Hist√≥rico** (primera ejecuci√≥n - ingesta todos los datos de junio-diciembre 2023):
```bash
python functions/weather_ingestion/main.py --historical
```

**Modo Diario** (ejecuci√≥n diaria - ingesta solo el d√≠a anterior):
```bash
python functions/weather_ingestion/main.py
```

### 5. Ejecutar Transformaciones dbt

```bash
cd dbt
dbt run --models silver
dbt run --models gold
dbt test
```

### 6. Crear Dashboard en Looker Studio

Sigue las instrucciones en [DASHBOARD_SETUP.md](DASHBOARD_SETUP.md) para crear el dashboard.

**Nota**: Una vez creado, compartir el dashboard con:
- alejandro@astrafy.io
- felipe.bereilh@orbidi.com

Y agregar el link en este README.

## Filtros de Datos

- **Per√≠odo de an√°lisis**: 01/06/2023 - 31/12/2023 (6 meses)
- **Fuente de taxis**: BigQuery Public Dataset `bigquery-public-data.chicago_taxi_trips.taxi_trips`
- **Datos del clima**: 
  - Fuente: Dataset p√∫blico de NOAA en BigQuery (bigquery-public-data.noaa_gsod)
  - Hist√≥ricos: 01/06/2023 - 31/12/2023 (6 meses, para mantener queries en tier gratuito)
  - Diarios: Se ingieren autom√°ticamente cada d√≠a (aunque no se usen en el dashboard)

## Pipeline de Datos

### Flujo de Datos

1. **Raw Layer (Bronze)**
   - Datos de taxis extra√≠dos de BigQuery p√∫blico
   - Datos del clima obtenidos desde dataset p√∫blico de NOAA en BigQuery

2. **Silver Layer**
   - Limpieza y deduplicaci√≥n de datos
   - Validaci√≥n de esquemas
   - Enriquecimiento con metadatos

3. **Gold Layer**
   - Agregaciones por d√≠a/hora
   - Joins entre taxis y clima
   - M√©tricas calculadas (duraci√≥n promedio, etc.)

### Automatizaci√≥n

- **GitHub Actions**: Despliega autom√°ticamente cuando haces push a `main`
  - CI: Valida c√≥digo en PRs
  - CD: Despliega infraestructura y ejecuta dbt en `main`
- **Cloud Scheduler**: Ejecuta la funci√≥n de ingesta diaria a las 02:00 AM UTC
- **Cloud Functions**: Procesa la ingesta de datos del clima
- **dbt**: Transformaciones ejecutadas manualmente o v√≠a CI/CD

**Nota sobre escalabilidad**: Para esta prueba, Cloud Scheduler es suficiente. Si en el futuro se requieren m√∫ltiples ingestas diarias, procesos en batch complejos, o dependencias entre tareas, se recomienda migrar a Cloud Composer (Apache Airflow) para una orquestaci√≥n m√°s robusta.

## Seguridad

- Column-level security implementada para `payment_type`
- Solo el email del desarrollador tiene acceso a esta columna
- Implementado mediante pol√≠ticas de seguridad de BigQuery

## CI/CD

El proyecto incluye pipelines de CI/CD con GitHub Actions para:
- Validaci√≥n de c√≥digo Terraform
- Tests de modelos dbt
- Linting y formateo de c√≥digo

## Costos

Al usar solo 6 meses de datos (junio-diciembre 2023), las consultas permanecen dentro del tier gratuito de Google Cloud para proyectos nuevos.

## Autor

Desarrollado como parte del desaf√≠o t√©cnico de Data Engineer de Orbidi.
