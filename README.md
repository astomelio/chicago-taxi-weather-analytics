# Orbidi Data Engineer - Chicago Taxi Trips Analysis

**Esta prueba tiene 2 puntos:**
- **Punto 1**: Documentaci√≥n y dise√±o ‚Üí ver `Part1_Architecture_Design.md`
- **Punto 2**: Implementaci√≥n t√©cnica ‚Üí se explica en este README

## Descripci√≥n del Proyecto

Este proyecto analiza la relaci√≥n entre las condiciones clim√°ticas y la duraci√≥n de los viajes en taxis de Chicago. El alcalde de Chicago sospecha que el clima afecta la duraci√≥n de los viajes, por lo que se ha desarrollado un dashboard en Looker Studio para explorar esta hip√≥tesis.

**Este repositorio implementa el Part 2: Coding Challenge del desaf√≠o t√©cnico de Orbidi.**

## Arquitectura

### Componentes Principales

1. **Ingesta de Datos**
   - **Datos de Taxis**: Extra√≠dos directamente de BigQuery (dataset p√∫blico de Chicago)
   - **Datos del Clima**: Obtenidos desde dataset p√∫blico de NOAA en BigQuery (bigquery-public-data.noaa_gsod)
   - Pipeline programado con Cloud Scheduler y Cloud Functions

2. **Almacenamiento**
   - **BigQuery**: Data warehouse para almacenar datos raw y transformados
   - Capas de datos: Raw (Bronze) ‚Üí Silver ‚Üí Gold

3. **Transformaci√≥n**
   - **dbt**: Herramienta para transformaciones y modelado de datos
   - Eliminaci√≥n de duplicados en capa Silver
   - Agregaciones y joins en capa Gold

4. **Visualizaci√≥n**
   - **Looker Studio**: Dashboard interactivo para an√°lisis

5. **Seguridad**
   - Column-level security para `payment_type` (solo accesible por el email del desarrollador)

## Estructura del Proyecto

```
.
‚îú‚îÄ‚îÄ terraform/              # Infraestructura como c√≥digo
‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   ‚îî‚îÄ‚îÄ modules/
‚îú‚îÄ‚îÄ dbt/                    # Transformaciones de datos
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ profiles.yml
‚îÇ   ‚îî‚îÄ‚îÄ dbt_project.yml
‚îú‚îÄ‚îÄ airflow/                # Orquestaci√≥n con Airflow (RECOMENDADO)
‚îÇ   ‚îî‚îÄ‚îÄ dags/
‚îÇ       ‚îú‚îÄ‚îÄ chicago_taxi_pipeline.py
‚îÇ       ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ functions/              # Cloud Functions
‚îÇ   ‚îî‚îÄ‚îÄ weather_ingestion/
‚îú‚îÄ‚îÄ scripts/               # Scripts auxiliares
‚îÇ   ‚îî‚îÄ‚îÄ cargar_historicos_via_gcs.sh
‚îú‚îÄ‚îÄ .github/               # CI/CD (solo para infraestructura)
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îî‚îÄ‚îÄ README.md
```

## üöÄ Inicio R√°pido - Todo Autom√°tico

**GitHub Actions hace TODO autom√°ticamente:**

1. **Configurar Secrets en GitHub**:
   - Ve a: `Settings > Secrets and variables > Actions`
   - Agrega: `GCP_SA_KEY`, `GCP_PROJECT_ID`, `DEVELOPER_EMAIL`
   - (Opcional): `OPENWEATHER_API_KEY`, `GCP_REGION`

2. **Hacer push a main**:
   ```bash
   git push origin main
   ```

3. **GitHub Actions autom√°ticamente**:
   - ‚úÖ Despliega infraestructura con Terraform (BigQuery, Cloud Functions, etc.)
   - ‚úÖ Crea entorno de Cloud Composer (si no existe)
   - ‚úÖ Sube DAGs de Airflow
   - ‚úÖ Sube c√≥digo dbt
   - ‚úÖ Configura variables de Airflow
   - ‚úÖ **TODO queda listo para usar**

4. **Cargar datos hist√≥ricos (una vez)**:
   - Ve a Airflow UI (el link aparece en los logs de GitHub Actions)
   - Trigger el DAG `chicago_taxi_historical_ingestion`
   - Espera a que complete (puede tardar 20-30 minutos)
   
   El pipeline diario se ejecutar√° autom√°ticamente despu√©s de cargar los hist√≥ricos

üìñ **Gu√≠a completa de Airflow**: Ver [airflow/README.md](airflow/README.md)

### ¬øQui√©n hace qu√©?

- **Terraform (via GitHub Actions)**: Crea infraestructura (BigQuery, Cloud Functions, Cloud Scheduler)
- **GitHub Actions**: Configura Airflow autom√°ticamente (crea Composer, sube DAGs, configura variables)
- **Airflow**: Ejecuta el pipeline de datos (ingesta, transformaciones dbt)
- **T√∫**: Solo necesitas trigger el DAG hist√≥rico una vez

---

## Requisitos Previos

- Cuenta de Google Cloud Platform con proyecto activo y facturaci√≥n habilitada
- Cuenta de GitHub (para despliegue autom√°tico)
- (Opcional) API key de clima solo si BigQuery p√∫blico no tiene datos para alguna fecha

## Configuraci√≥n Inicial

### Opci√≥n 1: Pipeline con Airflow (Recomendado para Datos)

Ver [airflow/README.md](airflow/README.md) para instrucciones completas.

### Opci√≥n 2: Despliegue de Infraestructura con GitHub Actions

**Pasos:**

1. **Crear Service Account en GCP** (ver `CONFIGURAR_GITHUB.md`)

2. **Configurar Secrets en GitHub**:
   - Ve a: `Settings > Secrets and variables > Actions`
   - Agrega: `GCP_SA_KEY`, `GCP_PROJECT_ID`, `DEVELOPER_EMAIL`
   - (Opcional): `OPENWEATHER_API_KEY`, `GCP_REGION`

3. **Hacer push a main**:
   ```bash
   git push origin main
   ```

**GitHub Actions autom√°ticamente:**
- ‚úÖ Habilita APIs necesarias
- ‚úÖ Crea ZIP de la funci√≥n
- ‚úÖ Ejecuta `terraform apply`
- ‚úÖ Despliega toda la infraestructura
- ‚úÖ Ejecuta modelos dbt

### Dashboard en Looker Studio

**Link del dashboard**: https://lookerstudio.google.com/s/qfSVoIMVddw  
En este link se encuentra el dashboard del proyecto que muestra el an√°lisis de la relaci√≥n entre clima y duraci√≥n de viajes.

**Nota**: Una vez creado, compartir el dashboard con:
- alejandro@astrafy.io
- felipe.bereilh@orbidi.com

Y agregar el link en este README.

## Filtros de Datos

- **Per√≠odo de an√°lisis**: 01/06/2023 - 31/12/2023 (6 meses)
- **Fuente de taxis**: BigQuery Public Dataset `bigquery-public-data.chicago_taxi_trips.taxi_trips`
- **Datos del clima**: 
  - Fuente: Dataset p√∫blico de NOAA en BigQuery (bigquery-public-data.noaa_gsod)
  - Hist√≥ricos: 01/06/2023 - 31/12/2023 (6 meses, para mantener queries en tier gratuito)
  - Diarios: Se ingieren autom√°ticamente cada d√≠a (aunque no se usen en el dashboard)

## Pipeline de Datos

### Flujo de Datos

1. **Raw Layer (Bronze)**
   - Datos de taxis extra√≠dos de BigQuery p√∫blico
   - Datos del clima obtenidos desde dataset p√∫blico de NOAA en BigQuery

2. **Silver Layer**
   - Limpieza y deduplicaci√≥n de datos
   - Validaci√≥n de esquemas
   - Enriquecimiento con metadatos

3. **Gold Layer**
   - Agregaciones por d√≠a/hora
   - Joins entre taxis y clima
   - M√©tricas calculadas (duraci√≥n promedio, etc.)

### Automatizaci√≥n

- **GitHub Actions**: Despliega autom√°ticamente cuando haces push a `main`
  - CI: Valida c√≥digo en PRs
  - CD: Despliega infraestructura y ejecuta dbt en `main`
- **Cloud Scheduler**: Ejecuta la funci√≥n de ingesta diaria a las 02:00 AM UTC
- **Cloud Functions**: Procesa la ingesta de datos del clima
- **dbt**: Transformaciones ejecutadas manualmente o v√≠a CI/CD

**Nota sobre escalabilidad**: Para esta prueba, Cloud Scheduler es suficiente. Si en el futuro se requieren m√∫ltiples ingestas diarias, procesos en batch complejos, o dependencias entre tareas, se recomienda migrar a Cloud Composer (Apache Airflow) para una orquestaci√≥n m√°s robusta.

## Seguridad

- Column-level security implementada para `payment_type`
- Solo el email del desarrollador tiene acceso a esta columna
- Implementado mediante pol√≠ticas de seguridad de BigQuery

## CI/CD

El proyecto incluye pipelines de CI/CD con GitHub Actions para:
- Validaci√≥n de c√≥digo Terraform
- Tests de modelos dbt
- Linting y formateo de c√≥digo

## Costos

Al usar solo 6 meses de datos (junio-diciembre 2023), las consultas permanecen dentro del tier gratuito de Google Cloud para proyectos nuevos.

## Estructura de la prueba

### Punto 1
- Documento y dise√±o en `Part1_Architecture_Design.md`
- Gu√≠a del diagrama en `Part1_Diagram_Guide.txt`

### Punto 2 (este README)
- C√≥digo en GitHub + Dashboard en Looker Studio
- An√°lisis de relaci√≥n entre clima y duraci√≥n de viajes en taxis de Chicago
- Estado: ‚úÖ Implementado y funcionando

#### Tablas y resultados (resumen)

**Tablas principales**
- **Raw**:
  - `chicago_taxi_raw.taxi_trips_raw_table` (taxis 2023-06 a 2023-12)
  - `chicago_taxi_raw.weather_data` (NOAA 2023-06 a 2023-12)
- **Silver**:
  - `chicago_taxi_silver.taxi_trips_silver`
  - `chicago_taxi_silver.weather_silver`
- **Gold**:
  - `chicago_taxi_gold.daily_summary`
  - `chicago_taxi_gold.taxi_weather_analysis`

**Conteos (aprox.)**
- `taxi_trips_silver`: **3,808,846**
- `weather_silver`: **214**
- `daily_summary`: **214**
- `taxi_weather_analysis`: **5,136**

**dbt tests**
- **Total**: 7 tests
- **Resultado**: ‚úÖ Todos pasan
- **Cobertura**: `not_null` y `unique` en claves y fechas principales

## Autor

Desarrollado para la prueba t√©cnica de Data Engineer de Orbidi.
